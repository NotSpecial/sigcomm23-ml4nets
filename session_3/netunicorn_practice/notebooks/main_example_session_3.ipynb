{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f0acbf-ad83-4b43-8a50-05aa65ae0b10",
   "metadata": {},
   "source": [
    "# Session 3: Hands-On Excercises - using netUnicorn in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e23673-9f3a-4a3e-913d-ccb36cd24ac8",
   "metadata": {},
   "source": [
    "In this session, we will implement an iterative approach to dataset collection using Trustee to analyze collected data and verify that our dataset doesn't have any obvious issues or shortcuts.\n",
    "\n",
    "We will implement one of tasks for our pipeline, combine tasks into a pipeline, create an experiment, and deploy it. After we will collect the data, we will explore it using XAI tools if any shortcuts or problems are presented, and will fix them and recollect the data to improve our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cd77c-2f2e-46a9-8f08-b27d636e0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# netunicorn.client is responsible for your connection to the netunicorn instance\n",
    "from netunicorn.client.remote import RemoteClient, RemoteClientException\n",
    "\n",
    "# netunicorn.base contains all \"building blocks\" to create Tasks, Pipelines, Experiments, etc. \n",
    "from netunicorn.base import Experiment, ExperimentStatus, Pipeline, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879f556-b4f5-4e2e-a889-16a763a515b4",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "\n",
    "Problem: classification between YouTube and Vimeo traffic for the same video using raw PCAPs.  \n",
    "Approach: watch YouTube and Vimeo, collect network traffic, mark flows as \"youtube\", \"vimeo\", \"other\", and try to build a classifier on top of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625042f-b96e-4c16-856d-92af75a6c84a",
   "metadata": {},
   "source": [
    "### Tasks implementation: ping\n",
    "Let's implement a simple ping task that would verify connectivity to both YouTube and Vimeo before we start doing anything. The task should do nothing during initialization except base class initialization, and implement ping with 3 packets to both \"youtube.com\" and \"vimeo.com\". If both pings finished successfully, return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b93516-26d8-468c-b732-0b48940f0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PingYouTubeAndVimeoTask(Task):\n",
    "    \"\"\"Pings YouTube and Vimeo and returns None if success\"\"\"\n",
    "\n",
    "    # we need to ensure that inetutils-ping package is installed in our Debian-based image\n",
    "    requirements = [\"apt install -y inetutils-ping\"]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # nothing interesting here\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self):\n",
    "        # implement actual ping logic here\n",
    "\n",
    "        # remove the line below when the implementation is finished\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71a46c-56c3-4ef1-ba45-8eba33bb7ab8",
   "metadata": {},
   "source": [
    "## Pipeline and Experiment creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb235e3-75bb-4662-b012-90bf0df48a42",
   "metadata": {},
   "source": [
    "Great! Now let's combine our pipeline.  \n",
    "\n",
    "Pipeline should:\n",
    " - Ping YouTube and Vimeo\n",
    " - Start tcpdump capture\n",
    " - Watch YouTube several times\n",
    " - Stop tcpdump capture\n",
    " - Start tcpdump capture again\n",
    " - Watch Vimeo several times\n",
    " - Stop tcpdump capture and save all files for local analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0da861-c16a-4125-bdad-3b9d666f0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import all other tasks to not reimplement them\n",
    "\n",
    "# Tasks to start tcpdump and stop named tcpdump task\n",
    "from netunicorn.library.tasks.capture.tcpdump import StartCaptureLinuxImplementation, StopNamedCaptureLinuxImplementation\n",
    "\n",
    "# Tasks for watching the corresponding video platform\n",
    "from netunicorn.library.tasks.video_watchers.youtube_watcher import WatchYouTubeVideoLinuxImplementation\n",
    "from netunicorn.library.tasks.video_watchers.vimeo_watcher import WatchVimeoVideoLinuxImplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094522e6-8014-41db-adf0-8909367a1f30",
   "metadata": {},
   "source": [
    "Now let's combine the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401e5db-1d32-45b2-9cf2-0b6b0f41e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline and removing early stopping so if any task fails pipeline would go on working\n",
    "pipeline = Pipeline()\n",
    "pipeline.early_stopping = False\n",
    "\n",
    "# ping youtube and vimeo\n",
    "pipeline.then(PingYouTubeAndVimeoTask())\n",
    "\n",
    "# starting tcpdump for youtube\n",
    "pipeline.then(StartCaptureLinuxImplementation(filepath=\"/tmp/capture_youtube.pcap\", name=\"capture_youtube\"))\n",
    "\n",
    "# watching youtube several times\n",
    "for _ in range(4):\n",
    "    pipeline.then([\n",
    "        WatchYouTubeVideoLinuxImplementation(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", 20),\n",
    "        WatchYouTubeVideoLinuxImplementation(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", 20),\n",
    "    ])\n",
    "\n",
    "# stopping tcpdump for youtube\n",
    "pipeline.then(StopNamedCaptureLinuxImplementation(capture_task_name=\"capture_youtube\"))\n",
    "\n",
    "# starting tcpdump for vimeo\n",
    "pipeline.then(StartCaptureLinuxImplementation(filepath=\"/tmp/capture_vimeo.pcap\", name=\"capture_vimeo\"))\n",
    "\n",
    "# watching vimeo\n",
    "for _ in range(3):\n",
    "    pipeline.then([\n",
    "        WatchVimeoVideoLinuxImplementation(\"https://vimeo.com/375468729\", 15),\n",
    "        WatchVimeoVideoLinuxImplementation(\"https://vimeo.com/375468729\", 15),\n",
    "    ])\n",
    "\n",
    "# stopping tcpdump for vimeo\n",
    "pipeline.then(StopNamedCaptureLinuxImplementation(capture_task_name=\"capture_vimeo\"))\n",
    "\n",
    "# let's print the resulting pipeline\n",
    "for element in pipeline.tasks:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4f315-9779-4da9-8b4c-532af2980da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a netunicorn instance deployed locally, so let's use it\n",
    "NETUNICORN_ENDPOINT = 'http://localhost:26611'\n",
    "NETUNICORN_LOGIN = 'test'\n",
    "NETUNICORN_PASSWORD = 'test'\n",
    "\n",
    "# create a client and check that connection and instance are ok\n",
    "client = RemoteClient(endpoint=NETUNICORN_ENDPOINT, login=NETUNICORN_LOGIN, password=NETUNICORN_PASSWORD)\n",
    "client.healthcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41731b-7ba2-45df-86ab-f0f75982adb7",
   "metadata": {},
   "source": [
    "Now let's ask for all available nodes and just take the first one.\n",
    "\n",
    "We're using local netunicorn instance for these experiments that just can deploy docker containers locally, but with other connectors you can connect to external Kubernetes, AWS, Azure, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99430cf6-a435-46e9-a5bd-b6e77b18d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = client.get_nodes()\n",
    "working_nodes = nodes.take(1)\n",
    "print(working_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a360b2-4455-4a9c-bbf8-ea0b3f7f7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the experiment - mapping our pipeline to all nodes\n",
    "experiment = Experiment().map(pipeline, working_nodes)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66240c-c634-4357-b0f1-09029dc5c777",
   "metadata": {},
   "source": [
    "By default, netunicorn will install all dependencies by itself, but let's use instead prepared docker image to speed up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03137585-6b74-41aa-88aa-e7dc1ff77e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netunicorn.base import DockerImage\n",
    "for deployment in experiment:\n",
    "    deployment.environment_definition = DockerImage(image='pinot.cs.ucsb.edu/sigcommtutorial:latest')  # set the required image\n",
    "    deployment.environment_definition.runtime_context.additional_arguments = [\"/tmp:/tmp\"]             # also mount the local folder to save files\n",
    "    deployment.cleanup = False                                                                         # and do not delete image afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28506404-7c75-4f82-9fd3-7c675bfcf5dd",
   "metadata": {},
   "source": [
    "Removing all potential previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1aa8f-2802-4823-9d5b-75772e7c8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/capture*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f406-9cba-426b-8bf0-c41fdb0005be",
   "metadata": {},
   "source": [
    "## Experiment preparation and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534872fc-0644-4ed7-8475-06469bfab40c",
   "metadata": {},
   "source": [
    "Now we have a prepared experiment - pipeline mapped to some nodes. Let's prepare and start it.\n",
    "\n",
    "Let's name our experiment somehow, delete previous execution if it existed, and ask netunicorn to prepare the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769c0d5-578b-4ee3-9361-ded2a18ec6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_label = \"session3-1\"\n",
    "\n",
    "try:\n",
    "    client.delete_experiment(experiment_label)\n",
    "except RemoteClientException:\n",
    "    pass\n",
    "\n",
    "client.prepare_experiment(experiment, experiment_label)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5ab7-2aeb-4418-8ae5-1f83f96f8de7",
   "metadata": {},
   "source": [
    "We will track preparation by periodically asking about status of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6179d-ee85-4eae-b8bf-84e58ced5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.PREPARING:\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c611672-3d17-4e09-8bf0-8a3feaf6d499",
   "metadata": {},
   "source": [
    "Let's check that all deployments are deployed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9131e-d70c-4087-a17e-1e3656d4f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for deployment in client.get_experiment_status(experiment_label).experiment:\n",
    "    print(f\"Prepared: {deployment.prepared}, error: {deployment.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26564fc1-9203-4790-9fbe-04192849e92c",
   "metadata": {},
   "source": [
    "Let's ask to start the execution and wait till experiment would be finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2a4f0-2d82-405b-bbd7-4d26979e04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.start_execution(experiment_label)\n",
    "\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.RUNNING:\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cc0a1-c11e-4ad9-b0e5-34a986b80399",
   "metadata": {},
   "source": [
    "Here's how we can get a full information about the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cbaf3-7f2f-4b6a-9629-0f4c59cd681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from returns.pipeline import is_successful\n",
    "\n",
    "for report in info.execution_result:\n",
    "    print(f\"Node name: {report.node.name}\")    # execution node name\n",
    "    print(f\"Error: {report.error}\")            # if any error happened\n",
    "\n",
    "    result, log = report.result  # report stores results of execution and corresponding log\n",
    "    \n",
    "    # result is a returns.result.Result object, could be Success of Failure\n",
    "    print(f\"Result is: {type(result)}\")\n",
    "\n",
    "    # let's unwrap the result (from the Success or Failure container to the actual result)\n",
    "    data = result.unwrap() if is_successful(result) else result.failure()\n",
    "\n",
    "    # and print all task names and corresponding execution results\n",
    "    for key, value in data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # we also can explore logs of the executor in case there's anything there\n",
    "    for line in log:\n",
    "        print(line.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff4cb8-645d-4fa2-bd2c-f461e9ade623",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af04c42-1575-4a4a-b815-cdc10df340cd",
   "metadata": {},
   "source": [
    "Now we have raw PCAPs with data and need to preprocess it to convert to some features we will work with.\n",
    "\n",
    "For this tutorial, we selected the CICFlowMeter format, which creates flow statistics features vectors from raw PCAPs. E.g., if inside your PCAP you have three connections (5-tuple flows), it will return a CSV with five rows and columns that contain this flow description (e.g., mean IAT, total length, number of packets, etc..)\n",
    "\n",
    "For preprocessing with CICFlowMeter, we will use the prepared docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731275c-2607-421c-a27c-ccf5300e9037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a CSV for youtube traffic\n",
    "!docker run -v /tmp/capture_youtube.pcap:/tmp/capture_youtube.pcap -v /tmp:/tmp/output --rm pinot.cs.ucsb.edu/cicflowmeter:latest /tmp/capture_youtube.pcap /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c252015-081e-4cfc-bcb0-28d5d6555816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a CSV for vimeo traffic\n",
    "!docker run -v /tmp/capture_vimeo.pcap:/tmp/capture_vimeo.pcap -v /tmp:/tmp/output --rm pinot.cs.ucsb.edu/cicflowmeter:latest /tmp/capture_vimeo.pcap /tmp/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f181a13-aee8-4bfa-ab4d-6d3d8145fb90",
   "metadata": {},
   "source": [
    "For data analysis, we will take resulting CSVs and create a random forest classif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f71775-ca5f-4f67-8ac8-85aaecd590e8",
   "metadata": {},
   "source": [
    "Now let's preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2da468-323a-47e1-a9f1-2aecc0fff73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube = pd.read_csv(\"/tmp/capture_youtube.pcap_Flow.csv\")\n",
    "df_vimeo = pd.read_csv(\"/tmp/capture_vimeo.pcap_Flow.csv\")\n",
    "\n",
    "print(df_youtube.columns)   # these are all columns that CICFlowMeter uses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f3f0c-2c39-49ba-94cc-58095c5a676e",
   "metadata": {},
   "source": [
    "To simplify the tutorial a bit and avoid dimensionality problems (when we have too many features for our dataset size) we will use a subset of features that represent a typical video streaming flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0e87c-68f3-44fa-8b75-7ac87e6342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Label\",\n",
    "    \"Protocol\",\n",
    "    \"Flow Duration\",\n",
    "    \"Flow Bytes/s\",\n",
    "    \"Flow Packets/s\",\n",
    "    \"Flow IAT Mean\",\n",
    "    \"Bwd IAT Mean\",\n",
    "    \"Down/Up Ratio\",\n",
    "    \"Active Mean\",\n",
    "    \"Idle Mean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d186b2-92cb-4e14-961a-6822b4e2e9de",
   "metadata": {},
   "source": [
    "Let's clean YouTube and Vimeo traffic. We will mark all connections with more than 30 forward or backward packets as video stream connections, and will drop extra UDP traffic not related to streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106239d5-192f-4709-a014-53778baedc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube['Label'] = 'other'\n",
    "df_youtube.loc[(df_youtube['Total Fwd Packet'] > 30) | (df_youtube['Total Bwd packets'] > 30), 'Label'] = 'youtube'  \n",
    "df_youtube = df_youtube.drop(df_youtube[(df_youtube['Protocol'] == 17) & (df_youtube['Label'] != 'youtube')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea943-2f11-48bf-befa-df531563160e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vimeo['Label'] = 'other'\n",
    "df_vimeo.loc[(df_vimeo['Total Fwd Packet'] > 30) | (df_vimeo['Total Bwd packets'] > 30), 'Label'] = 'vimeo'\n",
    "df_vimeo = df_vimeo.drop(df_vimeo[(df_vimeo['Protocol'] == 17) & (df_vimeo['Label'] != 'vimeo')].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eadb59-b608-4fa4-a495-c5a4ce16fb79",
   "metadata": {},
   "source": [
    "Now we can concat these two dataframes together and leave only features that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c853d4e-c0d6-4c27-b7d9-a460cbf8288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_youtube, df_vimeo], ignore_index=True)\n",
    "df = df[features]\n",
    "df = df.dropna()  # remove rows with Nones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d0e51-4f80-452a-9589-cdbec2524a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12373a31-9800-4047-87c0-3c5d3a41be33",
   "metadata": {},
   "source": [
    "## Classifier training\n",
    "Now let's train a random forest classifier based on features of our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2528d0-5ecf-4293-92bb-f2cec1b8041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc94acf-4a28-4c41-b5df-45e4fbf4a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data frame to features and answers\n",
    "target_variable = 'Label'\n",
    "train_features = list(set(df.columns) - {target_variable})\n",
    "x_train = df[train_features]\n",
    "y_train = df[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468fc2f-6f94-48fe-b4b1-5c072f6e34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and start training a classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee909f-96c9-4203-99bc-f88c0e579f57",
   "metadata": {},
   "source": [
    "Great, we have our classifier trained! Let's explore the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3d901-af76-4252-aace-ea80241c8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_train.values)\n",
    "print(metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764eea5-b870-4f23-b0cd-f6814e8e3035",
   "metadata": {},
   "source": [
    "Look suspicious :) Let's explain the actual model with Trustee and see the reasons of such performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c21c1-5ea0-4507-aee3-77ee5b6e7d86",
   "metadata": {},
   "source": [
    "## Classifier exploration and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1690b4-b50e-440c-81c6-0d98f9a6e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustee import ClassificationTrustee\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create and train a trustee tree\n",
    "trustee = ClassificationTrustee(expert=clf)\n",
    "trustee.fit(x_train, y_train, num_samples=len(x_train) // 2, num_iter=20, train_size=0.99)\n",
    "\n",
    "# print trustee explanation results\n",
    "_, dt, _, score = trustee.explain()\n",
    "print(f\"Training score of pruned DT: {score}\")\n",
    "dt_y_pred = dt.predict(x_train)\n",
    "\n",
    "print(\"Model explanation global fidelity report:\")\n",
    "print(metrics.classification_report(clf.predict(x_train), dt_y_pred))\n",
    "print(\"Model explanation score report:\")\n",
    "print(metrics.classification_report(y_train, dt_y_pred))\n",
    "\n",
    "# plot a tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plot_tree(dt, feature_names=x_train.columns, class_names=sorted(df['Label'].unique()), filled=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504f692-36a6-445c-81d5-ac83099f06c8",
   "metadata": {},
   "source": [
    "### Discussion on analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbfbf1-d1fd-4132-b92f-279dcda3cbb7",
   "metadata": {},
   "source": [
    "## Iteration #2 - fixing the dataset\n",
    "Change the pipeline to fix the problem - prohibit usage of QUIC efficiently removing protocol difference in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19647d2c-d541-4321-b535-d0ef2cd35dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipeline again\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.early_stopping = False\n",
    "\n",
    "pipeline.then(PingYouTubeAndVimeoTask())\n",
    "\n",
    "pipeline.then(StartCaptureLinuxImplementation(filepath=\"/tmp/capture_youtube.pcap\", name=\"capture_youtube\"))\n",
    "for _ in range(2):\n",
    "    pipeline.then([\n",
    "        WatchYouTubeVideoLinuxImplementation(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", 15, webdriver_arguments=[\"disable-quic\"]),    # notice \"disable-quic\"\n",
    "        WatchYouTubeVideoLinuxImplementation(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", 15, webdriver_arguments=[\"disable-quic\"]),\n",
    "    ])\n",
    "pipeline.then(StopNamedCaptureLinuxImplementation(capture_task_name=\"capture_youtube\"))\n",
    "\n",
    "pipeline.then(StartCaptureLinuxImplementation(filepath=\"/tmp/capture_vimeo.pcap\", name=\"capture_vimeo\"))\n",
    "for _ in range(2):\n",
    "    pipeline.then([\n",
    "        WatchVimeoVideoLinuxImplementation(\"https://vimeo.com/375468729\", 15),\n",
    "        WatchVimeoVideoLinuxImplementation(\"https://vimeo.com/375468729\", 15),\n",
    "    ])\n",
    "pipeline.then(StopNamedCaptureLinuxImplementation(capture_task_name=\"capture_vimeo\"))\n",
    "\n",
    "for element in pipeline.tasks:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2f1e7-fd70-463d-90e3-4593c557af6e",
   "metadata": {},
   "source": [
    "Again let's implement the experiment, deploy it, and run till completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88942902-6a25-4c7d-9654-8c6bdb492d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment().map(pipeline, working_nodes)\n",
    "\n",
    "for deployment in experiment:\n",
    "    deployment.environment_definition = DockerImage(image='pinot.cs.ucsb.edu/sigcommtutorial:latest')\n",
    "    deployment.environment_definition.runtime_context.additional_arguments = [\"/tmp:/tmp\"]\n",
    "    deployment.cleanup = False\n",
    "\n",
    "!rm -rf /tmp/capture*\n",
    "\n",
    "experiment_label = \"session3-2\"\n",
    "\n",
    "try:\n",
    "    client.delete_experiment(experiment_label)\n",
    "except RemoteClientException:\n",
    "    pass\n",
    "\n",
    "client.prepare_experiment(experiment, experiment_label)\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.PREPARING:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "for deployment in client.get_experiment_status(experiment_label).experiment:\n",
    "    print(f\"Prepared: {deployment.prepared}, error: {deployment.error}\")\n",
    "\n",
    "client.start_execution(experiment_label)\n",
    "\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_label)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.RUNNING:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "for report in info.execution_result:\n",
    "    print(f\"Node name: {report.node.name}\")\n",
    "    print(f\"Error: {report.error}\")\n",
    "\n",
    "    result, log = report.result  # report stores results of execution and corresponding log\n",
    "    \n",
    "    # result is a returns.result.Result object, could be Success of Failure\n",
    "    print(f\"Result is: {type(result)}\")\n",
    "    data = result.unwrap() if is_successful(result) else result.failure()\n",
    "    for key, value in data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # we also can explore logs\n",
    "    for line in log:\n",
    "        print(line.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adced4c-14bf-4a96-b988-5df4cf42cbc4",
   "metadata": {},
   "source": [
    "Again using CICFlowmeter to generate data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c67de6-8756-4c83-aa8f-aa064b764452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v /tmp/capture_youtube.pcap:/tmp/capture_youtube.pcap -v /tmp:/tmp/output --rm pinot.cs.ucsb.edu/cicflowmeter:latest /tmp/capture_youtube.pcap /tmp/output\n",
    "!docker run -v /tmp/capture_vimeo.pcap:/tmp/capture_vimeo.pcap -v /tmp:/tmp/output --rm pinot.cs.ucsb.edu/cicflowmeter:latest /tmp/capture_vimeo.pcap /tmp/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388c0ae-ef34-4708-8087-aed3c6a71772",
   "metadata": {},
   "source": [
    "Again absolutely the same procedure for data preparation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315f82b-0f8c-4c68-b6b0-cfe226e9e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube = pd.read_csv(\"/tmp/capture_youtube.pcap_Flow.csv\")\n",
    "df_vimeo = pd.read_csv(\"/tmp/capture_vimeo.pcap_Flow.csv\")\n",
    "\n",
    "df_youtube['Label'] = 'other'\n",
    "df_youtube.loc[(df_youtube['Total Fwd Packet'] > 30) | (df_youtube['Total Bwd packets'] > 30), 'Label'] = 'youtube'\n",
    "df_youtube = df_youtube.drop(df_youtube[(df_youtube['Protocol'] == 17) & (df_youtube['Label'] != 'youtube')].index)\n",
    "\n",
    "df_vimeo['Label'] = 'other'\n",
    "df_vimeo.loc[(df_vimeo['Total Fwd Packet'] > 30) | (df_vimeo['Total Bwd packets'] > 30), 'Label'] = 'vimeo'\n",
    "df_vimeo = df_vimeo.drop(df_vimeo[(df_vimeo['Protocol'] == 17) & (df_vimeo['Label'] != 'vimeo')].index)\n",
    "\n",
    "df = pd.concat([df_youtube, df_vimeo], ignore_index=True)\n",
    "df = df[features]\n",
    "df = df.dropna()\n",
    "\n",
    "target_variable = 'Label'\n",
    "features = list(set(df.columns) - {target_variable})\n",
    "x_train = df[features]\n",
    "y_train = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12472ccc-8991-40d0-93c1-bb65029c5677",
   "metadata": {},
   "source": [
    "And also training a classifier, explaining it with Trustee, and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab8cc8-5e7a-4c28-9bb6-32f2346daf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_train.values)\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "trustee = ClassificationTrustee(expert=clf)\n",
    "trustee.fit(x_train, y_train, num_samples=len(x_train) // 2, num_iter=20, train_size=0.99)\n",
    "\n",
    "_, dt, _, score = trustee.explain()\n",
    "print(f\"Training score of pruned DT: {score}\")\n",
    "dt_y_pred = dt.predict(x_train)\n",
    "\n",
    "print(\"Model explanation global fidelity report:\")\n",
    "print(metrics.classification_report(clf.predict(x_train), dt_y_pred))\n",
    "print(\"Model explanation score report:\")\n",
    "print(metrics.classification_report(y_train, dt_y_pred))\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plot_tree(dt, feature_names=x_train.columns, class_names=sorted(df['Label'].unique()), filled=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8809d5-efee-48a8-916a-4a9403c30ccb",
   "metadata": {},
   "source": [
    "## Iteration #2 - results discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c11060-12e4-42bd-b78a-bd1b065f4932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
