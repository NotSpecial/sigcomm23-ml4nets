{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic netunicorn experiment example\n",
    "This example shows basic client-side usage of netunicorn API.\n",
    "Prerequisites:\n",
    "- overall understanding of the project\n",
    "- deployed netunicorn infrastructure and director services\n",
    "- known `endpoint`, `login`, and `password` for the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the project, you need to install several packages:\n",
    "- `netunicorn-base` - provides abstractions and classes to create pipelines and define experiments. If you want to just define your pipeline and write tasks, you need only this package.\n",
    "- `netunicorn-client` - provides connectivity to netunicorn infrastructure. You need this package to submit and execute experiments.\n",
    "- `netunicorn-library` - a library of predefined and contributed tasks for the platform. You can use tasks in this package for your pipelines, and submit your code here to share. Please note, that most of the tasks there are provided 'as-is' by other teams and developers, and netunicorn team doesn't guarantee their correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netunicorn-base in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (0.3.6)\n",
      "Requirement already satisfied: returns in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base) (4.7.1)\n",
      "Requirement already satisfied: pydantic in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.1 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: netunicorn-client in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: netunicorn-base>=0.3.3 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-client) (0.3.6)\n",
      "Requirement already satisfied: cloudpickle in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-client) (2.2.1)\n",
      "Requirement already satisfied: requests in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-client) (2.31.0)\n",
      "Requirement already satisfied: returns in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-client) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base>=0.3.3->netunicorn-client) (4.7.1)\n",
      "Requirement already satisfied: pydantic in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base>=0.3.3->netunicorn-client) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from requests->netunicorn-client) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from requests->netunicorn-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from requests->netunicorn-client) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from requests->netunicorn-client) (2023.7.22)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base>=0.3.3->netunicorn-client) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.1 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base>=0.3.3->netunicorn-client) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: netunicorn-library in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: netunicorn-base>=0.3.1 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-library) (0.3.6)\n",
      "Requirement already satisfied: returns in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base>=0.3.1->netunicorn-library) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base>=0.3.1->netunicorn-library) (4.7.1)\n",
      "Requirement already satisfied: pydantic in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from netunicorn-base>=0.3.1->netunicorn-library) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base>=0.3.1->netunicorn-library) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.1 in /home/kell/miniconda3/envs/sigcomm-tutorial/lib/python3.11/site-packages (from pydantic->netunicorn-base>=0.3.1->netunicorn-library) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install netunicorn-base\n",
    "%pip install netunicorn-client\n",
    "%pip install netunicorn-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import needed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# for pretty printing of JSONs\n",
    "from pprint import pprint\n",
    "\n",
    "# client to connect to the infrastructure\n",
    "from netunicorn.client.remote import RemoteClient, RemoteClientException\n",
    "\n",
    "# basic abstraction for experiment creation and management\n",
    "from netunicorn.base.experiment import Experiment, ExperimentStatus\n",
    "from netunicorn.base.pipeline import Pipeline\n",
    "\n",
    "# task to be executed in the pipeline\n",
    "# you can write your own tasks, but now let's use simple predefined one\n",
    "from netunicorn.library.tasks.basic import SleepTask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we want to define a pipeline to execute.\n",
    "\n",
    "Pipelines consist of tasks located on different stages, and would be executed by each node where they would be assigned later.\n",
    "\n",
    "To create a pipeline, instantiate Pipeline object and use the method `.then()` to define a new stage. All tasks on the same stage would be started together, and the next stage would start only when all tasks from the current stage successfully finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will use simple SleepTask for this example\n",
    "# you can look at the source code of the SleepTaskLinuxImplementation in netunicorn-library to understand how it works\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Notice, that executor will first in parallel execute `sleep 5` and `sleep 3`...\n",
    "pipeline = pipeline.then([\n",
    "    SleepTask(5),\n",
    "    SleepTask(3)\n",
    "]).then(\n",
    "    # ...and after they finished (after 5 second in total) will execute `sleep 10`\n",
    "    SleepTask(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine multiple tasks and stages to create your own pipelines. Each instance of a task in a pipeline would be serialized (together with all variables) and sent to the executor. The result of task (and pipeline in general) execution would be serialized and sent back to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment is defined as one or multiple assignments of pipelines to particular nodes. To define an experiment, we should get available nodes in infrastructure.\n",
    "\n",
    "To access the infrastructure, you need several parameters, provided by netunicorn deployment administrators (e.g., your university or company). In the next cell we will read these parameters from environment variables or use default values if they are not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you have .env file locally for storing credentials\n",
    "if '.env' in os.listdir():\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\".env\")\n",
    "\n",
    "# API connection endpoint\n",
    "endpoint = os.environ.get('NETUNICORN_ENDPOINT') or 'http://localhost:26611'\n",
    "\n",
    "# user login\n",
    "login = os.environ.get('NETUNICORN_LOGIN') or 'test'\n",
    "\n",
    "# user password\n",
    "password = os.environ.get('NETUNICORN_PASSWORD') or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a client with these parameters\n",
    "client = RemoteClient(endpoint=endpoint, login=login, password=password)\n",
    "client.healthcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using client, you can receive information about available nodes, and then filter them and take needed amount for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's receive all available for us nodes\n",
    "nodes = client.get_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dockerhost]\n"
     ]
    }
   ],
   "source": [
    "# let's explore existing nodes\n",
    "for element in nodes:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** if you are executing this notebook with another instance of netunicorn, your available nodes would differ from the ones shown here. Feel free to skip the next several cells or modify them to fit your infrastructure.\n",
    "\n",
    "Here we meet the first important concept of netunicorn - node pools.\n",
    "\n",
    "Node pools are objects that represent existing nodes in the infrastructure. Generally, there exists two types of node pools:\n",
    "- CountableNodePool - represents a set of nodes with a fixed number of nodes. Usually used for static infrastructures with real physical nodes.\n",
    "- UncountableNodePool - represents a set of nodes where nodes are created dynamically. Typical examples of such infrastructures are cloud providers, such as AWS, GCP, Azure, etc.\n",
    "\n",
    "CountableNodePool can contain other node pools, while Uncountable cannot. When you request for nodes from the client, you always receive a CountableNodePool, that can contain 0 or more other pools. Let's verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "netunicorn.base.nodes.CountableNodePool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both CountableNodePool and UncountableNodePool have unified interface, but some methods could work differently. For example, we can use \\_\\_getitem__ to get a node (or another pool) from CountableNodePool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netunicorn.base.nodes.CountableNodePool'>\n",
      "[dockerhost]\n"
     ]
    }
   ],
   "source": [
    "node_pool = nodes[0]\n",
    "print(type(node_pool))\n",
    "print(node_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, UncountableNodePool have infinite number of nodes. So instead, this method would return a _template_ of a node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dockerhost"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_pool[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this difference, we suggest to use the next methods to work with nodes from pools:\n",
    "- `take` - returns Sequence of N nodes from the pool (or generate N nodes, if it is UncountableNodePool)\n",
    "- `skip` - returns a new pool without N first nodes (or pools)\n",
    "- `filter` - accepts boolean function and returns a new pool with nodes (or pools) that satisfy this function\n",
    "\n",
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dockerhost]\n",
      "\n",
      "Take three first nodes: [dockerhost]\n",
      "Skip first object in the `nodes` pool and take three next nodes: []\n",
      "Filter example: []\n"
     ]
    }
   ],
   "source": [
    "for element in nodes:\n",
    "    print(element)\n",
    "\n",
    "print()\n",
    "print(f\"Take three first nodes: {nodes.take(3)}\")\n",
    "print(f\"Skip first object in the `nodes` pool and take three next nodes: {nodes.skip(1).take(3)}\")  # notice, that we skipped the whole node pool\n",
    "print(f\"Filter example: {nodes.filter(lambda node: node.name.startswith('raspi-') or node.name.startswith('aws-fargate-A-'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that in the `filter` example we received a new pool with two node pools inside, each containing nodes satisfying the condition.\n",
    "\n",
    "Let's explore the node object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dockerhost\n",
      "{'netunicorn-environments': ['DockerImage'], 'connector': 'defaultdocker'}\n",
      "Architecture.LINUX_AMD64\n"
     ]
    }
   ],
   "source": [
    "example_node = nodes.take(5)[0] \n",
    "print(example_node.name)  # unique identifier of the node\n",
    "print(example_node.properties)  # different properties that could be used for filtering\n",
    "print(example_node.architecture)  # architecture of the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any part of the node for filtering if needed. For full Node description, please refer to the documentation.\n",
    "\n",
    "Let's select several real world Raspberry Pi nodes for our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "interesting_nodes = nodes.filter(lambda node: node.name.startswith('raspi-'))  # feel free to change it if needed\n",
    "working_nodes = interesting_nodes.take(3)\n",
    "print(working_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell for those who playing with this notebook on a local infrastructure:\n",
    "if os.environ.get('NETUNICORN_ENDPOINT', 'http://localhost:26611')  == 'http://localhost:26611':\n",
    "    working_nodes = client.get_nodes().take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, our first experiment would consist of all nodes running the same pipeline. Let's create Experiment instance and add all nodes with the pipeline using `map()` method. You can read the documentation about other methods of creating assignments (called `Deployments` in netunicorn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Deployment: Node=dockerhost, executor_id=, prepared=False\n",
      "\n",
      "dockerhost\n",
      "DockerImage(commands=[], runtime_context=RuntimeContext(ports_mapping={}, environment_variables={}, additional_arguments=[]), image=None, build_context=BuildContext(python_version='3.11.0', cloudpickle_version='2.2.1'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment().map(pipeline, working_nodes)\n",
    "\n",
    "# let's explore experiment object\n",
    "print(experiment)\n",
    "print()\n",
    "\n",
    "# and separate Deployments\n",
    "for deployment in experiment:\n",
    "    print(deployment.node)\n",
    "    print(deployment.environment_definition)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice, that each deployment has `environment definition` - important part of the deployment, that stores environment variables, port mappings, decide where pipeline would be executed, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit the experiment, we need to create a user-wide unique name for the experiment, and call an appropriate method of the `client` object. Notice, that you can submit the same experiment several times with different names to be executed more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_cool_name'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'experiment_cool_name'\n",
    "try:\n",
    "    # just in case you already have this experiment\n",
    "    client.delete_experiment(experiment_name)\n",
    "except RemoteClientException as e:\n",
    "    # if not, exception would be fired\n",
    "    pass\n",
    "\n",
    "client.prepare_experiment(experiment, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you submit an experiment, netunicorn services automatically create or download a virtual environment for execution of the pipeline, insert serialized pipeline inside and distribute these environments to the desired nodes.\n",
    "\n",
    "To check status of the experiment, you can use corresponding method of the `client` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperimentStatus.PREPARING\n",
      "ExperimentStatus.PREPARING\n",
      "ExperimentStatus.READY\n"
     ]
    }
   ],
   "source": [
    "# status will change from PREPARING to READY when compiled and deployed\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_name)\n",
    "    print(info.status)\n",
    "    if info.status == ExperimentStatus.READY:\n",
    "        break\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we used default settings for deployments, during the execution of the previous cell, netunicorn:\n",
    "1. Extracted all requirements for tasks and analyzed nodes \n",
    "2. Compiled new Docker images for each combination of pipeline and architecture\n",
    "3. Distributed these Docker images to experiment nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node name: dockerhost\n",
      "Deployed correctly: True\n",
      "Error: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One of the returned objects is a prepared experiment. It holds all the information about deployments compilation\n",
    "# Some nodes could be failed to prepare due to various reasons\n",
    "prepared_experiment = info.experiment\n",
    "for deployment in prepared_experiment:\n",
    "    print(f\"Node name: {deployment.node}\")\n",
    "    print(f\"Deployed correctly: {deployment.prepared}\")\n",
    "    print(f\"Error: {deployment.error}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the status is ready, nodes are prepared for execution and downloaded all the needed environments and pipelines (don't forget to check `prepared` status of the returned experiment to confirm).\n",
    "\n",
    "Now you can ask `client` object to start the experiment. It will ask nodes to spin up executors and will collect the execution results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_cool_name'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.start_execution(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperimentStatus.RUNNING\n",
      "ExperimentStatus.FINISHED\n"
     ]
    }
   ],
   "source": [
    "# Again, let's check experiment status until it changes from RUNNING to FINISHED\n",
    "import time\n",
    "while True:\n",
    "    info = client.get_experiment_status(experiment_name)\n",
    "    print(info.status)\n",
    "    if info.status != ExperimentStatus.RUNNING:\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the experiment execution, netunicorn:\n",
    "1. Started Docker containers on each node of the experiment\n",
    "2. Each container started `netunicorn-executor`, that loaded the pipeline and executed it.\n",
    "3. During the execution, `executor` periodically notified `director` services with `heartbeat` messages, that it is alive and working\n",
    "4. After pipeline execution, `executor` uploaded results to the netunicorn and finalized the environment\n",
    "5. After all executors finished, `director` services collected results and finalized the experiment\n",
    "\n",
    "Let's explore the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node name: dockerhost\n",
      "Error: None\n",
      "<class 'returns.result.Success'>\n",
      "defaultdict(<class 'list'>,\n",
      "            {'07a2d766-9ea4-43f3-ab18-31b5ab2663c5': [<Success: 3>],\n",
      "             '5a150171-6087-4b83-b795-7acfbfa024c9': [<Success: 10>],\n",
      "             'db6e3a81-4130-40bb-a643-319636aaa3e9': [<Success: 5>]})\n",
      "Parsed configuration: Gateway located on http://gateway:26512\n",
      "Current directory: /\n",
      "Pipeline loaded from local file, executing.\n",
      "Pipeline finished, start reporting results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from returns.pipeline import is_successful\n",
    "\n",
    "for report in info.execution_result:\n",
    "    print(f\"Node name: {report.node.name}\")\n",
    "    print(f\"Error: {report.error}\")\n",
    "\n",
    "    result, log = report.result  # report stores results of execution and corresponding log\n",
    "    \n",
    "    # result is a returns.result.Result object, could be Success of Failure\n",
    "    print(type(result))\n",
    "    data = result.unwrap() if is_successful(result) else result\n",
    "    pprint(data)\n",
    "\n",
    "    # we also can explore logs\n",
    "    for line in log:\n",
    "        print(line.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! \n",
    "\n",
    "We (hope, you too ^_^) successfully finished the basic experiment using the netunicorn platform.\n",
    "\n",
    "For next steps, you can read the documentation on creating more complex experiments, including writing your own tasks, providing your own Docker containers, experiment synchronization, etc.\n",
    "\n",
    "For all questions, refer to the official organization: https://github.com/netunicorn and netunicorn team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigcomm-tutorial",
   "language": "python",
   "name": "sigcomm-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
