{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4-n3jVUVET"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "* Use `pip` to install dependencies.\n",
        "* Install `graphviz` to render decision trees.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b0twAF-HRztd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4ec1fe9-be2e-43cf-e551-3a3fa8a0773d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (57.5.0)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-68.1.2-py3-none-any.whl (805 kB)\n",
            "Requirement already satisfied: jedi in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi) (0.8.3)\n",
            "Installing collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.5.0\n",
            "    Uninstalling setuptools-57.5.0:\n",
            "      Successfully uninstalled setuptools-57.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trustee 1.1.4 requires setuptools<58.0.0,>=57.0.0, but you have setuptools 68.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-68.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: trustee==1.1.4 in /usr/local/lib/python3.10/dist-packages (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.2.0)\n",
            "Requirement already satisfied: furo<2023.0.0,>=2022.6.21 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (2022.12.7)\n",
            "Requirement already satisfied: graphviz>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (0.20.1)\n",
            "Requirement already satisfied: prettytable==3.0.0 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (3.0.0)\n",
            "Collecting setuptools<58.0.0,>=57.0.0 (from trustee==1.1.4)\n",
            "  Using cached setuptools-57.5.0-py3-none-any.whl (819 kB)\n",
            "Requirement already satisfied: sphinx-gallery<0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (0.11.1)\n",
            "Requirement already satisfied: sphinxemoji<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (0.2.0)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from trustee==1.1.4) (1.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable==3.0.0->trustee==1.1.4) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (4.11.2)\n",
            "Requirement already satisfied: sphinx<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (5.0.2)\n",
            "Requirement already satisfied: sphinx-basic-ng in /usr/local/lib/python3.10/dist-packages (from furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.0.0b2)\n",
            "Requirement already satisfied: pygments>=2.7 in /usr/local/lib/python3.10/dist-packages (from furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.16.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.0.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (3.1.2)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<7.0,>=5.0->furo<2023.0.0,>=2022.6.21->trustee==1.1.4) (2023.7.22)\n",
            "Installing collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 68.1.2\n",
            "    Uninstalling setuptools-68.1.2:\n",
            "      Successfully uninstalled setuptools-68.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 57.5.0 which is incompatible.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 57.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-57.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade setuptools jedi\n",
        "!pip install matplotlib numpy pandas scikit-learn==1.2.2 scipy trustee==1.1.4\n",
        "!apt -qqq install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFIKYCUWJI3"
      },
      "source": [
        "# Traditional ML Pipeline\n",
        "\n",
        "* Read CIC-IDS-2017 dataset\n",
        "  * The CIC-IDS-2017 dataset is hosted in the Github repo from the project.\n",
        "\n",
        "* To read the data, we are using a helper function from the Trustee package.\n",
        "  * This method automatically one-hot encodes the categorical variables of the dataset, based on the provided metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ukt1OVI3WafH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4873df16-a5c0-4acc-cf4b-be8d8a523d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Metadata start. ==========\n",
            "Names: ['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n",
            "Dummies: [30, 31]\n",
            "Use cols: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]\n",
            "Result variable: [75]\n",
            "========== Metadata end. ==========\n",
            "\n",
            "Pandas read_csv complete.\n",
            "CSV dataset read:\n",
            "        Destination Port  Flow Duration  Total Fwd Packets  \\\n",
            "0                     80       84300154                  5   \n",
            "1                     80         618990                  3   \n",
            "2                     80      106029730                  5   \n",
            "3                     53            221                  2   \n",
            "4                     80      101927345                  7   \n",
            "...                  ...            ...                ...   \n",
            "340959                80       86064113                  8   \n",
            "340960                80      105641775                 16   \n",
            "340961                80        5915952                  3   \n",
            "340962                80            193                  2   \n",
            "340963                80       11493636                  8   \n",
            "\n",
            "        Total Backward Packets  Total Length of Fwd Packets  \\\n",
            "0                            7                          358   \n",
            "1                            4                           26   \n",
            "2                            4                          247   \n",
            "3                            2                           64   \n",
            "4                            6                          341   \n",
            "...                        ...                          ...   \n",
            "340959                       6                          350   \n",
            "340960                       3                         2541   \n",
            "340961                       1                            0   \n",
            "340962                       0                            0   \n",
            "340963                       6                          632   \n",
            "\n",
            "        Total Length of Bwd Packets  Fwd Packet Length Max  \\\n",
            "0                             11595                    346   \n",
            "1                             11607                     20   \n",
            "2                                 0                    231   \n",
            "3                               204                     32   \n",
            "4                             11595                    341   \n",
            "...                             ...                    ...   \n",
            "340959                        11595                    350   \n",
            "340960                            6                    231   \n",
            "340961                            0                      0   \n",
            "340962                            0                      0   \n",
            "340963                        11632                    632   \n",
            "\n",
            "        Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  \\\n",
            "0                           0               71.625000             153.375000   \n",
            "1                           0                8.664062              10.265625   \n",
            "2                           0               49.406250             101.625000   \n",
            "3                          32               32.000000               0.000000   \n",
            "4                           0               48.718750             128.875000   \n",
            "...                       ...                     ...                    ...   \n",
            "340959                      0               43.750000             123.750000   \n",
            "340960                      0              158.750000             110.562500   \n",
            "340961                      0                0.000000               0.000000   \n",
            "340962                      0                0.000000               0.000000   \n",
            "340963                      0               79.000000             223.500000   \n",
            "\n",
            "        ...  min_seg_size_forward  Active Mean    Active Std  Active Max  \\\n",
            "0       ...                    20      11008.0  0.000000e+00       11008   \n",
            "1       ...                    20          0.0  0.000000e+00           0   \n",
            "2       ...                    32        483.5  5.579072e+02         878   \n",
            "3       ...                    32          0.0  0.000000e+00           0   \n",
            "4       ...                    32          3.0  0.000000e+00           3   \n",
            "...     ...                   ...          ...           ...         ...   \n",
            "340959  ...                    32          5.0  0.000000e+00           5   \n",
            "340960  ...                    32    4709590.5  6.659740e+06     9418738   \n",
            "340961  ...                    32          0.0  0.000000e+00           0   \n",
            "340962  ...                    32          0.0  0.000000e+00           0   \n",
            "340963  ...                    32       1404.0  0.000000e+00        1404   \n",
            "\n",
            "        Active Min    Idle Mean    Idle Std   Idle Max   Idle Min  Label  \n",
            "0            11008   84100000.0         0.0   84100000   84100000      4  \n",
            "1                0          0.0         0.0          0          0      2  \n",
            "2               89   53000000.0  66400000.0  100000000    6029218      6  \n",
            "3                0          0.0         0.0          0          0      0  \n",
            "4                3  102000000.0         0.0  102000000  102000000      4  \n",
            "...            ...          ...         ...        ...        ...    ...  \n",
            "340959           5   85100000.0         0.0   85100000   85100000      4  \n",
            "340960         443   19200000.0  18800000.0   51300000    5957569      6  \n",
            "340961           0          0.0         0.0          0          0     14  \n",
            "340962           0          0.0         0.0          0          0      4  \n",
            "340963        1404    6490512.0         0.0    6490512    6490512      3  \n",
            "\n",
            "[340964 rows x 76 columns]\n",
            "(340964, 76)\n",
            "Any NAN? 0\n",
            "Total memory usage: 69.26 MB\n",
            "Average memory usage: 0.90 MB\n",
            "Features Shape: (340964, 77)\n",
            "Column names:\n",
            "0: Destination Port\n",
            "1: Flow Duration\n",
            "2: Total Fwd Packets\n",
            "3: Total Backward Packets\n",
            "4: Total Length of Fwd Packets\n",
            "5: Total Length of Bwd Packets\n",
            "6: Fwd Packet Length Max\n",
            "7: Fwd Packet Length Min\n",
            "8: Fwd Packet Length Mean\n",
            "9: Fwd Packet Length Std\n",
            "10: Bwd Packet Length Max\n",
            "11: Bwd Packet Length Min\n",
            "12: Bwd Packet Length Mean\n",
            "13: Bwd Packet Length Std\n",
            "14: Flow Bytes/s\n",
            "15: Flow Packets/s\n",
            "16: Flow IAT Mean\n",
            "17: Flow IAT Std\n",
            "18: Flow IAT Max\n",
            "19: Flow IAT Min\n",
            "20: Fwd IAT Total\n",
            "21: Fwd IAT Mean\n",
            "22: Fwd IAT Std\n",
            "23: Fwd IAT Max\n",
            "24: Fwd IAT Min\n",
            "25: Bwd IAT Total\n",
            "26: Bwd IAT Mean\n",
            "27: Bwd IAT Std\n",
            "28: Bwd IAT Max\n",
            "29: Bwd IAT Min\n",
            "30: Bwd Header Length\n",
            "31: Fwd Packets/s\n",
            "32: Bwd Packets/s\n",
            "33: Min Packet Length\n",
            "34: Max Packet Length\n",
            "35: Packet Length Mean\n",
            "36: Packet Length Std\n",
            "37: Packet Length Variance\n",
            "38: FIN Flag Count\n",
            "39: SYN Flag Count\n",
            "40: RST Flag Count\n",
            "41: PSH Flag Count\n",
            "42: ACK Flag Count\n",
            "43: URG Flag Count\n",
            "44: CWE Flag Count\n",
            "45: ECE Flag Count\n",
            "46: Down/Up Ratio\n",
            "47: Average Packet Size\n",
            "48: Avg Fwd Segment Size\n",
            "49: Avg Bwd Segment Size\n",
            "50: Fwd Header Length\n",
            "51: Fwd Avg Bytes/Bulk\n",
            "52: Fwd Avg Packets/Bulk\n",
            "53: Fwd Avg Bulk Rate\n",
            "54: Bwd Avg Bytes/Bulk\n",
            "55: Bwd Avg Packets/Bulk\n",
            "56: Bwd Avg Bulk Rate\n",
            "57: Subflow Fwd Packets\n",
            "58: Subflow Fwd Bytes\n",
            "59: Subflow Bwd Packets\n",
            "60: Subflow Bwd Bytes\n",
            "61: Init_Win_bytes_forward\n",
            "62: Init_Win_bytes_backward\n",
            "63: act_data_pkt_fwd\n",
            "64: min_seg_size_forward\n",
            "65: Active Mean\n",
            "66: Active Std\n",
            "67: Active Max\n",
            "68: Active Min\n",
            "69: Idle Mean\n",
            "70: Idle Std\n",
            "71: Idle Max\n",
            "72: Idle Min\n",
            "73: Fwd PSH Flags_0\n",
            "74: Fwd PSH Flags_1\n",
            "75: Fwd URG Flags_0\n",
            "76: Fwd URG Flags_1\n",
            "\n",
            "Targets shape: (340964, 1) Index(['Label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from trustee.utils import dataset\n",
        "from trustee.utils.const import CIC_IDS_2017_DATASET_META\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DF_PATH = \"https://github.com/TrusteeML/emperor/raw/main/use_cases/heartbleed_case/res/dataset/CIC-IDS-2017_OverSampled_min.csv.zip\"\n",
        "\n",
        "# if using oversampled df\n",
        "CIC_IDS_2017_DATASET_META[\"is_dir\"] = False\n",
        "\n",
        "# Step 1: Parse train-test def\n",
        "X, y, feature_names, _, _ = dataset.read(\n",
        "    DF_PATH, metadata=CIC_IDS_2017_DATASET_META, as_df=True, verbose=True\n",
        ")\n",
        "\n",
        "# select subsample of the dataset\n",
        "idx = np.random.permutation(X.index)\n",
        "idx = idx[:int(len(idx) * 0.1)]\n",
        "\n",
        "X = X.loc[idx]\n",
        "y = y.loc[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVWgUSEZ0l0B"
      },
      "source": [
        "Split dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zEhJS-6K0o3B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_indexes = np.arange(0, X.shape[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_indexes, y, train_size=0.7, stratify=y)\n",
        "X_train = X.iloc[X_train]\n",
        "X_test = X.iloc[X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skmZt9pqZfR4"
      },
      "source": [
        "Train Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QjwHtDq7ZkF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abf8141-d9de-472a-e67a-3336f284e309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blackbox model classification report with IID:\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "                  BENIGN      0.981     0.983     0.982       689\n",
            "                     Bot      0.993     0.994     0.993       680\n",
            "                    DDoS      0.999     1.000     0.999       674\n",
            "           DoS GoldenEye      0.996     0.996     0.996       675\n",
            "                DoS Hulk      1.000     0.994     0.997       665\n",
            "        DoS Slowhttptest      0.997     0.990     0.993       690\n",
            "           DoS slowloris      0.993     0.994     0.993       675\n",
            "             FTP-Patator      1.000     1.000     1.000       692\n",
            "              Heartbleed      1.000     1.000     1.000       663\n",
            "            Infiltration      1.000     1.000     1.000       672\n",
            "                PortScan      0.999     1.000     0.999       684\n",
            "             SSH-Patator      1.000     0.999     0.999       674\n",
            "  Web Attack Brute Force      0.960     0.900     0.929       689\n",
            "Web Attack Sql Injection      0.999     1.000     0.999       690\n",
            "          Web Attack XSS      0.907     0.967     0.936       717\n",
            "\n",
            "                accuracy                          0.987     10229\n",
            "               macro avg      0.988     0.988     0.988     10229\n",
            "            weighted avg      0.988     0.987     0.987     10229\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "blackbox = RandomForestClassifier(n_jobs=4)\n",
        "blackbox.fit(X_train, y_train)\n",
        "\n",
        "y_pred = blackbox.predict(X_test)\n",
        "\n",
        "print(\"Blackbox model classification report with IID:\")\n",
        "print(\n",
        "    \"\\n{}\".format(\n",
        "        classification_report(\n",
        "            y_test,\n",
        "            y_pred,\n",
        "            digits=3,\n",
        "            target_names=CIC_IDS_2017_DATASET_META[\"classes\"],\n",
        "        )\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_6ykJfB5pSm"
      },
      "source": [
        "# Trustee\n",
        "Run Classification Trustee on trained Random Forest Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bn1l9GW55t_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a21b9c-7bd8-456f-e343-ea230c06987a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 2 iterations\n",
            "########## Outer-loop Iteration 0/2 ##########\n",
            "Initializing Trustee inner-loop with 2 iterations\n",
            "########## Inner-loop Iteration 0/30 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 25 and 207 leaves:\n",
            "Student model score: 0.9488792648902211\n",
            "Student model 0-0 fidelity: 0.9488792648902211\n",
            "########## Inner-loop Iteration 1/30 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 25 and 186 leaves:\n",
            "Student model score: 0.940621548133763\n",
            "Student model 0-1 fidelity: 0.940621548133763\n",
            "########## Inner-loop Iteration 2/30 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 26 and 210 leaves:\n",
            "Student model score: 0.9523335980523702\n",
            "Student model 0-2 fidelity: 0.9523335980523702\n",
            "########## Inner-loop Iteration 3/30 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 0-3 trained with depth 24 and 197 leaves:\n",
            "Student model score: 0.9520867657393152\n",
            "Student model 0-3 fidelity: 0.9520867657393152\n",
            "########## Inner-loop Iteration 4/30 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 0-4 trained with depth 28 and 204 leaves:\n",
            "Student model score: 0.9425102242780923\n",
            "Student model 0-4 fidelity: 0.9425102242780923\n",
            "########## Inner-loop Iteration 5/30 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 0-5 trained with depth 23 and 188 leaves:\n",
            "Student model score: 0.9537589929375407\n",
            "Student model 0-5 fidelity: 0.9537589929375407\n",
            "########## Inner-loop Iteration 6/30 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 0-6 trained with depth 25 and 203 leaves:\n",
            "Student model score: 0.9566148572297827\n",
            "Student model 0-6 fidelity: 0.9566148572297827\n",
            "########## Inner-loop Iteration 7/30 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 0-7 trained with depth 27 and 200 leaves:\n",
            "Student model score: 0.9455001071978028\n",
            "Student model 0-7 fidelity: 0.9455001071978028\n",
            "########## Inner-loop Iteration 8/30 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 0-8 trained with depth 27 and 213 leaves:\n",
            "Student model score: 0.945574250526415\n",
            "Student model 0-8 fidelity: 0.945574250526415\n",
            "########## Inner-loop Iteration 9/30 ##########\n",
            "Sampling 5011 points from training dataset with (30242, 30242) entries\n",
            "Student model 0-9 trained with depth 23 and 189 leaves:\n",
            "Student model score: 0.95102695516208\n",
            "Student model 0-9 fidelity: 0.95102695516208\n",
            "########## Inner-loop Iteration 10/30 ##########\n",
            "Sampling 5011 points from training dataset with (31746, 31746) entries\n",
            "Student model 0-10 trained with depth 31 and 189 leaves:\n",
            "Student model score: 0.9479395969583123\n",
            "Student model 0-10 fidelity: 0.9479395969583123\n",
            "########## Inner-loop Iteration 11/30 ##########\n",
            "Sampling 5011 points from training dataset with (33250, 33250) entries\n",
            "Student model 0-11 trained with depth 24 and 187 leaves:\n",
            "Student model score: 0.9549903502642737\n",
            "Student model 0-11 fidelity: 0.9549903502642737\n",
            "########## Inner-loop Iteration 12/30 ##########\n",
            "Sampling 5011 points from training dataset with (34754, 34754) entries\n",
            "Student model 0-12 trained with depth 19 and 181 leaves:\n",
            "Student model score: 0.9547138104454884\n",
            "Student model 0-12 fidelity: 0.9547138104454884\n",
            "########## Inner-loop Iteration 13/30 ##########\n",
            "Sampling 5011 points from training dataset with (36258, 36258) entries\n",
            "Student model 0-13 trained with depth 23 and 182 leaves:\n",
            "Student model score: 0.9522685282869863\n",
            "Student model 0-13 fidelity: 0.9522685282869863\n",
            "########## Inner-loop Iteration 14/30 ##########\n",
            "Sampling 5011 points from training dataset with (37762, 37762) entries\n",
            "Student model 0-14 trained with depth 28 and 198 leaves:\n",
            "Student model score: 0.9575538131405286\n",
            "Student model 0-14 fidelity: 0.9575538131405286\n",
            "########## Inner-loop Iteration 15/30 ##########\n",
            "Sampling 5011 points from training dataset with (39266, 39266) entries\n",
            "Student model 0-15 trained with depth 22 and 187 leaves:\n",
            "Student model score: 0.9561711123207767\n",
            "Student model 0-15 fidelity: 0.9561711123207767\n",
            "########## Inner-loop Iteration 16/30 ##########\n",
            "Sampling 5011 points from training dataset with (40770, 40770) entries\n",
            "Student model 0-16 trained with depth 24 and 197 leaves:\n",
            "Student model score: 0.9507787335291613\n",
            "Student model 0-16 fidelity: 0.9507787335291613\n",
            "########## Inner-loop Iteration 17/30 ##########\n",
            "Sampling 5011 points from training dataset with (42274, 42274) entries\n",
            "Student model 0-17 trained with depth 22 and 190 leaves:\n",
            "Student model score: 0.9558927192160964\n",
            "Student model 0-17 fidelity: 0.9558927192160964\n",
            "########## Inner-loop Iteration 18/30 ##########\n",
            "Sampling 5011 points from training dataset with (43778, 43778) entries\n",
            "Student model 0-18 trained with depth 26 and 199 leaves:\n",
            "Student model score: 0.9437080656372834\n",
            "Student model 0-18 fidelity: 0.9437080656372834\n",
            "########## Inner-loop Iteration 19/30 ##########\n",
            "Sampling 5011 points from training dataset with (45282, 45282) entries\n",
            "Student model 0-19 trained with depth 24 and 180 leaves:\n",
            "Student model score: 0.9507185819821069\n",
            "Student model 0-19 fidelity: 0.9507185819821069\n",
            "########## Inner-loop Iteration 20/30 ##########\n",
            "Sampling 5011 points from training dataset with (46786, 46786) entries\n",
            "Student model 0-20 trained with depth 24 and 165 leaves:\n",
            "Student model score: 0.9496016997301117\n",
            "Student model 0-20 fidelity: 0.9496016997301117\n",
            "########## Inner-loop Iteration 21/30 ##########\n",
            "Sampling 5011 points from training dataset with (48290, 48290) entries\n",
            "Student model 0-21 trained with depth 24 and 188 leaves:\n",
            "Student model score: 0.9537203696184177\n",
            "Student model 0-21 fidelity: 0.9537203696184177\n",
            "########## Inner-loop Iteration 22/30 ##########\n",
            "Sampling 5011 points from training dataset with (49794, 49794) entries\n",
            "Student model 0-22 trained with depth 28 and 190 leaves:\n",
            "Student model score: 0.9576594079855705\n",
            "Student model 0-22 fidelity: 0.9576594079855705\n",
            "########## Inner-loop Iteration 23/30 ##########\n",
            "Sampling 5011 points from training dataset with (51298, 51298) entries\n",
            "Student model 0-23 trained with depth 21 and 186 leaves:\n",
            "Student model score: 0.9530742840082712\n",
            "Student model 0-23 fidelity: 0.9530742840082712\n",
            "########## Inner-loop Iteration 24/30 ##########\n",
            "Sampling 5011 points from training dataset with (52802, 52802) entries\n",
            "Student model 0-24 trained with depth 22 and 189 leaves:\n",
            "Student model score: 0.9563706699976502\n",
            "Student model 0-24 fidelity: 0.9563706699976502\n",
            "########## Inner-loop Iteration 25/30 ##########\n",
            "Sampling 5011 points from training dataset with (54306, 54306) entries\n",
            "Student model 0-25 trained with depth 20 and 176 leaves:\n",
            "Student model score: 0.9603588142134821\n",
            "Student model 0-25 fidelity: 0.9603588142134821\n",
            "########## Inner-loop Iteration 26/30 ##########\n",
            "Sampling 5011 points from training dataset with (55810, 55810) entries\n",
            "Student model 0-26 trained with depth 22 and 185 leaves:\n",
            "Student model score: 0.953247041107325\n",
            "Student model 0-26 fidelity: 0.953247041107325\n",
            "########## Inner-loop Iteration 27/30 ##########\n",
            "Sampling 5011 points from training dataset with (57314, 57314) entries\n",
            "Student model 0-27 trained with depth 22 and 178 leaves:\n",
            "Student model score: 0.9533943791163845\n",
            "Student model 0-27 fidelity: 0.9533943791163845\n",
            "########## Inner-loop Iteration 28/30 ##########\n",
            "Sampling 5011 points from training dataset with (58818, 58818) entries\n",
            "Student model 0-28 trained with depth 26 and 184 leaves:\n",
            "Student model score: 0.9619563083631247\n",
            "Student model 0-28 fidelity: 0.9619563083631247\n",
            "########## Inner-loop Iteration 29/30 ##########\n",
            "Sampling 5011 points from training dataset with (60322, 60322) entries\n",
            "Student model 0-29 trained with depth 22 and 175 leaves:\n",
            "Student model score: 0.952041681221989\n",
            "Student model 0-29 fidelity: 0.952041681221989\n",
            "########## Outer-loop Iteration 1/2 ##########\n",
            "Initializing Trustee inner-loop with 2 iterations\n",
            "########## Inner-loop Iteration 0/30 ##########\n",
            "Sampling 5011 points from training dataset with (61826, 61826) entries\n",
            "Student model 1-0 trained with depth 23 and 179 leaves:\n",
            "Student model score: 0.9459689264381906\n",
            "Student model 1-0 fidelity: 0.9459689264381906\n",
            "########## Inner-loop Iteration 1/30 ##########\n",
            "Sampling 5011 points from training dataset with (63330, 63330) entries\n",
            "Student model 1-1 trained with depth 25 and 182 leaves:\n",
            "Student model score: 0.9548926802115746\n",
            "Student model 1-1 fidelity: 0.9548926802115746\n",
            "########## Inner-loop Iteration 2/30 ##########\n",
            "Sampling 5011 points from training dataset with (64834, 64834) entries\n",
            "Student model 1-2 trained with depth 25 and 182 leaves:\n",
            "Student model score: 0.951404387985695\n",
            "Student model 1-2 fidelity: 0.951404387985695\n",
            "########## Inner-loop Iteration 3/30 ##########\n",
            "Sampling 5011 points from training dataset with (66338, 66338) entries\n",
            "Student model 1-3 trained with depth 22 and 175 leaves:\n",
            "Student model score: 0.9533519563709952\n",
            "Student model 1-3 fidelity: 0.9533519563709952\n",
            "########## Inner-loop Iteration 4/30 ##########\n",
            "Sampling 5011 points from training dataset with (67842, 67842) entries\n",
            "Student model 1-4 trained with depth 22 and 188 leaves:\n",
            "Student model score: 0.9557904576966548\n",
            "Student model 1-4 fidelity: 0.9557904576966548\n",
            "########## Inner-loop Iteration 5/30 ##########\n",
            "Sampling 5011 points from training dataset with (69346, 69346) entries\n",
            "Student model 1-5 trained with depth 25 and 187 leaves:\n",
            "Student model score: 0.9609838332947133\n",
            "Student model 1-5 fidelity: 0.9609838332947133\n",
            "########## Inner-loop Iteration 6/30 ##########\n",
            "Sampling 5011 points from training dataset with (70850, 70850) entries\n",
            "Student model 1-6 trained with depth 19 and 166 leaves:\n",
            "Student model score: 0.9663176112621936\n",
            "Student model 1-6 fidelity: 0.9663176112621936\n",
            "########## Inner-loop Iteration 7/30 ##########\n",
            "Sampling 5011 points from training dataset with (72354, 72354) entries\n",
            "Student model 1-7 trained with depth 26 and 191 leaves:\n",
            "Student model score: 0.9578590374780096\n",
            "Student model 1-7 fidelity: 0.9578590374780096\n",
            "########## Inner-loop Iteration 8/30 ##########\n",
            "Sampling 5011 points from training dataset with (73858, 73858) entries\n",
            "Student model 1-8 trained with depth 22 and 180 leaves:\n",
            "Student model score: 0.9369857427992438\n",
            "Student model 1-8 fidelity: 0.9369857427992438\n",
            "########## Inner-loop Iteration 9/30 ##########\n",
            "Sampling 5011 points from training dataset with (75362, 75362) entries\n",
            "Student model 1-9 trained with depth 23 and 181 leaves:\n",
            "Student model score: 0.9474188179115062\n",
            "Student model 1-9 fidelity: 0.9474188179115062\n",
            "########## Inner-loop Iteration 10/30 ##########\n",
            "Sampling 5011 points from training dataset with (76866, 76866) entries\n",
            "Student model 1-10 trained with depth 27 and 177 leaves:\n",
            "Student model score: 0.9501592407368661\n",
            "Student model 1-10 fidelity: 0.9501592407368661\n",
            "########## Inner-loop Iteration 11/30 ##########\n",
            "Sampling 5011 points from training dataset with (78370, 78370) entries\n",
            "Student model 1-11 trained with depth 21 and 162 leaves:\n",
            "Student model score: 0.9420996166876608\n",
            "Student model 1-11 fidelity: 0.9420996166876608\n",
            "########## Inner-loop Iteration 12/30 ##########\n",
            "Sampling 5011 points from training dataset with (79874, 79874) entries\n",
            "Student model 1-12 trained with depth 22 and 186 leaves:\n",
            "Student model score: 0.9571298505609052\n",
            "Student model 1-12 fidelity: 0.9571298505609052\n",
            "########## Inner-loop Iteration 13/30 ##########\n",
            "Sampling 5011 points from training dataset with (81378, 81378) entries\n",
            "Student model 1-13 trained with depth 25 and 187 leaves:\n",
            "Student model score: 0.9537888252177781\n",
            "Student model 1-13 fidelity: 0.9537888252177781\n",
            "########## Inner-loop Iteration 14/30 ##########\n",
            "Sampling 5011 points from training dataset with (82882, 82882) entries\n",
            "Student model 1-14 trained with depth 25 and 176 leaves:\n",
            "Student model score: 0.9529933545445063\n",
            "Student model 1-14 fidelity: 0.9529933545445063\n",
            "########## Inner-loop Iteration 15/30 ##########\n",
            "Sampling 5011 points from training dataset with (84386, 84386) entries\n",
            "Student model 1-15 trained with depth 20 and 183 leaves:\n",
            "Student model score: 0.9503594861076151\n",
            "Student model 1-15 fidelity: 0.9503594861076151\n",
            "########## Inner-loop Iteration 16/30 ##########\n",
            "Sampling 5011 points from training dataset with (85890, 85890) entries\n",
            "Student model 1-16 trained with depth 28 and 190 leaves:\n",
            "Student model score: 0.9615525212380966\n",
            "Student model 1-16 fidelity: 0.9615525212380966\n",
            "########## Inner-loop Iteration 17/30 ##########\n",
            "Sampling 5011 points from training dataset with (87394, 87394) entries\n",
            "Student model 1-17 trained with depth 26 and 186 leaves:\n",
            "Student model score: 0.9573127876481897\n",
            "Student model 1-17 fidelity: 0.9573127876481897\n",
            "########## Inner-loop Iteration 18/30 ##########\n",
            "Sampling 5011 points from training dataset with (88898, 88898) entries\n",
            "Student model 1-18 trained with depth 24 and 183 leaves:\n",
            "Student model score: 0.9422572770768628\n",
            "Student model 1-18 fidelity: 0.9422572770768628\n",
            "########## Inner-loop Iteration 19/30 ##########\n",
            "Sampling 5011 points from training dataset with (90402, 90402) entries\n",
            "Student model 1-19 trained with depth 24 and 177 leaves:\n",
            "Student model score: 0.9505969411953823\n",
            "Student model 1-19 fidelity: 0.9505969411953823\n",
            "########## Inner-loop Iteration 20/30 ##########\n",
            "Sampling 5011 points from training dataset with (91906, 91906) entries\n",
            "Student model 1-20 trained with depth 21 and 186 leaves:\n",
            "Student model score: 0.9570806971408299\n",
            "Student model 1-20 fidelity: 0.9570806971408299\n",
            "########## Inner-loop Iteration 21/30 ##########\n",
            "Sampling 5011 points from training dataset with (93410, 93410) entries\n",
            "Student model 1-21 trained with depth 24 and 187 leaves:\n",
            "Student model score: 0.9599574626738211\n",
            "Student model 1-21 fidelity: 0.9599574626738211\n",
            "########## Inner-loop Iteration 22/30 ##########\n",
            "Sampling 5011 points from training dataset with (94914, 94914) entries\n",
            "Student model 1-22 trained with depth 25 and 170 leaves:\n",
            "Student model score: 0.9632800089877961\n",
            "Student model 1-22 fidelity: 0.9632800089877961\n",
            "########## Inner-loop Iteration 23/30 ##########\n",
            "Sampling 5011 points from training dataset with (96418, 96418) entries\n",
            "Student model 1-23 trained with depth 23 and 167 leaves:\n",
            "Student model score: 0.9603855537683367\n",
            "Student model 1-23 fidelity: 0.9603855537683367\n",
            "########## Inner-loop Iteration 24/30 ##########\n",
            "Sampling 5011 points from training dataset with (97922, 97922) entries\n",
            "Student model 1-24 trained with depth 22 and 177 leaves:\n",
            "Student model score: 0.9600173173116807\n",
            "Student model 1-24 fidelity: 0.9600173173116807\n",
            "########## Inner-loop Iteration 25/30 ##########\n",
            "Sampling 5011 points from training dataset with (99426, 99426) entries\n",
            "Student model 1-25 trained with depth 25 and 177 leaves:\n",
            "Student model score: 0.9643103775401428\n",
            "Student model 1-25 fidelity: 0.9643103775401428\n",
            "########## Inner-loop Iteration 26/30 ##########\n",
            "Sampling 5011 points from training dataset with (100930, 100930) entries\n",
            "Student model 1-26 trained with depth 25 and 191 leaves:\n",
            "Student model score: 0.9559973520974236\n",
            "Student model 1-26 fidelity: 0.9559973520974236\n",
            "########## Inner-loop Iteration 27/30 ##########\n",
            "Sampling 5011 points from training dataset with (102434, 102434) entries\n",
            "Student model 1-27 trained with depth 25 and 189 leaves:\n",
            "Student model score: 0.9521365901421035\n",
            "Student model 1-27 fidelity: 0.9521365901421035\n",
            "########## Inner-loop Iteration 28/30 ##########\n",
            "Sampling 5011 points from training dataset with (103938, 103938) entries\n",
            "Student model 1-28 trained with depth 27 and 166 leaves:\n",
            "Student model score: 0.9550889385068636\n",
            "Student model 1-28 fidelity: 0.9550889385068636\n",
            "########## Inner-loop Iteration 29/30 ##########\n",
            "Sampling 5011 points from training dataset with (105442, 105442) entries\n",
            "Student model 1-29 trained with depth 22 and 183 leaves:\n",
            "Student model score: 0.9559973773676463\n",
            "Student model 1-29 fidelity: 0.9559973773676463\n",
            "Model explanation training (agreement, fidelity): (0.8751537421262994, 0.9619563083631247)\n",
            "Model Explanation size: 367\n",
            "Top-k Prunned Model explanation size: 43\n"
          ]
        }
      ],
      "source": [
        "import graphviz\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import datasets\n",
        "\n",
        "from trustee import ClassificationTrustee\n",
        "\n",
        "# Initialize Trustee and fit for classification models\n",
        "trustee = ClassificationTrustee(expert=blackbox)\n",
        "trustee.fit(X_train, y_train, num_iter=30, num_stability_iter=2, samples_size=0.3, verbose=True)\n",
        "\n",
        "# Get the best explanation from Trustee\n",
        "dt, pruned_dt, agreement, reward = trustee.explain(top_k=10)\n",
        "print(f\"Model explanation training (agreement, fidelity): ({agreement}, {reward})\")\n",
        "print(f\"Model Explanation size: {dt.tree_.node_count}\")\n",
        "print(f\"Top-k Prunned Model explanation size: {pruned_dt.tree_.node_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUk82pal_j7I"
      },
      "source": [
        "Evaluate explanations produced by Trustee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsfneYdK_nDB",
        "outputId": "5d8f359c-faab-4c19-e423-905b33bbe2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model explanation global fidelity report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.93       690\n",
            "           1       0.98      1.00      0.99       681\n",
            "           2       1.00      1.00      1.00       675\n",
            "           3       0.98      0.97      0.97       675\n",
            "           4       0.98      0.98      0.98       661\n",
            "           5       0.98      0.98      0.98       685\n",
            "           6       0.94      0.97      0.96       676\n",
            "           7       1.00      1.00      1.00       692\n",
            "           8       1.00      1.00      1.00       663\n",
            "           9       1.00      1.00      1.00       672\n",
            "          10       0.99      0.98      0.99       685\n",
            "          11       1.00      1.00      1.00       673\n",
            "          12       0.67      0.68      0.68       646\n",
            "          13       0.98      1.00      0.99       691\n",
            "          14       0.72      0.70      0.71       764\n",
            "\n",
            "    accuracy                           0.94     10229\n",
            "   macro avg       0.94      0.94      0.94     10229\n",
            "weighted avg       0.94      0.94      0.94     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.76      0.59       690\n",
            "           1       0.86      1.00      0.93       681\n",
            "           2       1.00      1.00      1.00       675\n",
            "           3       0.97      0.63      0.77       675\n",
            "           4       0.00      0.00      0.00       661\n",
            "           5       0.93      0.67      0.78       685\n",
            "           6       0.43      0.83      0.57       676\n",
            "           7       0.98      1.00      0.99       692\n",
            "           8       1.00      1.00      1.00       663\n",
            "           9       1.00      1.00      1.00       672\n",
            "          10       0.97      0.90      0.93       685\n",
            "          11       0.98      1.00      0.99       673\n",
            "          12       0.00      0.00      0.00       646\n",
            "          13       0.92      1.00      0.96       691\n",
            "          14       0.54      0.95      0.69       764\n",
            "\n",
            "    accuracy                           0.79     10229\n",
            "   macro avg       0.74      0.78      0.75     10229\n",
            "weighted avg       0.74      0.79      0.75     10229\n",
            "\n",
            "Model explanation score report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.90      0.93       689\n",
            "           1       0.97      0.99      0.98       680\n",
            "           2       0.99      1.00      1.00       674\n",
            "           3       0.98      0.97      0.98       675\n",
            "           4       0.98      0.98      0.98       665\n",
            "           5       0.97      0.97      0.97       690\n",
            "           6       0.94      0.97      0.95       675\n",
            "           7       1.00      1.00      1.00       692\n",
            "           8       1.00      1.00      1.00       663\n",
            "           9       1.00      1.00      1.00       672\n",
            "          10       0.99      0.98      0.98       684\n",
            "          11       1.00      1.00      1.00       674\n",
            "          12       0.69      0.66      0.68       689\n",
            "          13       0.98      1.00      0.99       690\n",
            "          14       0.67      0.70      0.69       717\n",
            "\n",
            "    accuracy                           0.94     10229\n",
            "   macro avg       0.94      0.94      0.94     10229\n",
            "weighted avg       0.94      0.94      0.94     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.75      0.59       689\n",
            "           1       0.86      1.00      0.93       680\n",
            "           2       0.99      1.00      1.00       674\n",
            "           3       0.97      0.63      0.76       675\n",
            "           4       0.00      0.00      0.00       665\n",
            "           5       0.93      0.67      0.78       690\n",
            "           6       0.43      0.83      0.56       675\n",
            "           7       0.98      1.00      0.99       692\n",
            "           8       1.00      1.00      1.00       663\n",
            "           9       1.00      1.00      1.00       672\n",
            "          10       0.97      0.90      0.93       684\n",
            "          11       0.98      1.00      0.99       674\n",
            "          12       0.00      0.00      0.00       689\n",
            "          13       0.92      1.00      0.96       690\n",
            "          14       0.51      0.94      0.66       717\n",
            "\n",
            "    accuracy                           0.78     10229\n",
            "   macro avg       0.73      0.78      0.74     10229\n",
            "weighted avg       0.73      0.78      0.74     10229\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use explanations to make predictions\n",
        "dt_y_pred = dt.predict(X_test)\n",
        "pruned_dt_y_pred = pruned_dt.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy and fidelity of explanations\n",
        "print(\"Model explanation global fidelity report:\")\n",
        "print(classification_report(y_pred, dt_y_pred))\n",
        "print(\"Top-k Model explanation global fidelity report:\")\n",
        "print(classification_report(y_pred, pruned_dt_y_pred))\n",
        "\n",
        "print(\"Model explanation score report:\")\n",
        "print(classification_report(y_test, dt_y_pred))\n",
        "print(\"Top-k Model explanation score report:\")\n",
        "print(classification_report(y_test, pruned_dt_y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zma8tqHh_vpv"
      },
      "source": [
        "Render Decision Tree explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9v6M1iBK_uCL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54135884-4200-4b52-d643-90e20046c7e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pruned_dt_explation.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\n",
        "# Output decision tree to pdf\n",
        "dot_data = tree.export_graphviz(\n",
        "    dt,\n",
        "    class_names=CIC_IDS_2017_DATASET_META[\"classes\"],\n",
        "    feature_names=feature_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True,\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"dt_explanation\")\n",
        "\n",
        "# Output pruned decision tree to pdf\n",
        "dot_data = tree.export_graphviz(\n",
        "    pruned_dt,\n",
        "    class_names=CIC_IDS_2017_DATASET_META[\"classes\"],\n",
        "    feature_names=feature_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True,\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"pruned_dt_explation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XGH1gzn9eXT"
      },
      "source": [
        "# Validation\n",
        "\n",
        "Read Validation dataset with Out-of-Distribution samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UHTj9nbJ9kk-"
      },
      "outputs": [],
      "source": [
        "VALIDATION_DF_PATH = \"https://raw.githubusercontent.com/TrusteeML/emperor/main/use_cases/heartbleed_case/res/dataset/validation/heartbleed.csv\"\n",
        "\n",
        "X_validate, y_validate, _, _, _ = dataset.read(VALIDATION_DF_PATH, metadata=CIC_IDS_2017_DATASET_META, as_df=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt4KM9FR_3ZA"
      },
      "source": [
        "Use Validation dataset to evaluate trained Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CA6KU3Ss_6aU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a453062-0cbe-4225-b0cb-993da8e914c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blackbox model classification report with OOD:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      BENIGN      0.000     0.000     0.000       0.0\n",
            "  Heartbleed      0.000     0.000     0.000    1000.0\n",
            "\n",
            "    accuracy                          0.000    1000.0\n",
            "   macro avg      0.000     0.000     0.000    1000.0\n",
            "weighted avg      0.000     0.000     0.000    1000.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_val_pred = blackbox.predict(X_validate)\n",
        "\n",
        "print(\"Blackbox model classification report with OOD:\")\n",
        "print(\n",
        "    \"\\n{}\".format(\n",
        "        classification_report(\n",
        "            y_validate,\n",
        "            y_val_pred,\n",
        "            digits=3,\n",
        "            target_names=[\"BENIGN\", \"Heartbleed\"],\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBgcAicvZwW-"
      },
      "source": [
        "# Trust Report\n",
        "\n",
        "Run Trust Report on trained Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iQS0hhSxZ_x9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a94308-1724-454e-de92-6bf56ae04712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Trust Report...\n",
            "Preparing data...\n",
            "Splitting dataset for training and testing...\n",
            "X size: 34096; y size: 34096\n",
            "Done!\n",
            "Done!\n",
            "\rProgress |█████-----------------------------------------------------------------------------------------------| 5.3% Complete\r\n",
            "Collecting blackbox information...\n",
            "Done!\n",
            "\rProgress |██████████------------------------------------------------------------------------------------------| 10.5% Complete\r\n",
            "Collecting trustee information...\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.989     0.979     0.984       703\n",
            "           1      0.993     1.000     0.996       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.989     0.998     0.994       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.997     0.999     0.998       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.996     0.998       712\n",
            "          12      0.967     0.878     0.920       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.896     0.974     0.933       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.988     10229\n",
            "weighted avg      0.988     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 22 and 209 leaves:\n",
            "Student model score: 0.9430916258814678\n",
            "Student model 0-0 fidelity: 0.9430916258814678\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 24 and 199 leaves:\n",
            "Student model score: 0.9412615123222968\n",
            "Student model 0-1 fidelity: 0.9412615123222968\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 24 and 211 leaves:\n",
            "Student model score: 0.952349573810293\n",
            "Student model 0-2 fidelity: 0.952349573810293\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 24 and 202 leaves:\n",
            "Student model score: 0.944458458736553\n",
            "Student model 1-0 fidelity: 0.944458458736553\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 24 and 207 leaves:\n",
            "Student model score: 0.9535638060675233\n",
            "Student model 1-1 fidelity: 0.9535638060675233\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 25 and 186 leaves:\n",
            "Student model score: 0.9492655721084832\n",
            "Student model 1-2 fidelity: 0.9492655721084832\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 27 and 222 leaves:\n",
            "Student model score: 0.9506276563356799\n",
            "Student model 2-0 fidelity: 0.9506276563356799\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 25 and 205 leaves:\n",
            "Student model score: 0.9516484952383563\n",
            "Student model 2-1 fidelity: 0.9516484952383563\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 26 and 198 leaves:\n",
            "Student model score: 0.9550453389458108\n",
            "Student model 2-2 fidelity: 0.9550453389458108\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.884204234518636, 0.9550453389458108)\n",
            "Top-k Prunned explanation size: 47\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.945     0.871     0.907       696\n",
            "           1      0.973     0.997     0.985       692\n",
            "           2      0.992     0.995     0.994       638\n",
            "           3      0.970     0.974     0.972       658\n",
            "           4      0.969     0.958     0.963       644\n",
            "           5      0.985     0.997     0.991       671\n",
            "           6      0.976     0.972     0.974       703\n",
            "           7      0.997     0.997     0.997       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.994     0.982     0.988       706\n",
            "          11      0.985     1.000     0.992       709\n",
            "          12      0.639     0.680     0.659       603\n",
            "          13      0.972     1.000     0.986       702\n",
            "          14      0.733     0.705     0.719       780\n",
            "\n",
            "    accuracy                          0.941     10229\n",
            "   macro avg      0.942     0.942     0.942     10229\n",
            "weighted avg      0.942     0.941     0.941     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.471     0.386     0.425       696\n",
            "           1      0.873     1.000     0.932       692\n",
            "           2      0.992     0.995     0.994       638\n",
            "           3      0.758     0.872     0.811       658\n",
            "           4      0.649     0.705     0.676       644\n",
            "           5      0.867     0.729     0.792       671\n",
            "           6      0.926     0.815     0.867       703\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.964     0.922     0.943       706\n",
            "          11      0.985     1.000     0.992       709\n",
            "          12      0.538     0.070     0.123       603\n",
            "          13      0.918     1.000     0.957       702\n",
            "          14      0.564     0.944     0.706       780\n",
            "\n",
            "    accuracy                          0.836     10229\n",
            "   macro avg      0.832     0.829     0.814     10229\n",
            "weighted avg      0.832     0.836     0.818     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.941     0.858     0.897       703\n",
            "           1      0.966     0.997     0.981       687\n",
            "           2      0.991     0.994     0.992       638\n",
            "           3      0.967     0.974     0.970       656\n",
            "           4      0.967     0.966     0.966       638\n",
            "           5      0.984     0.991     0.987       674\n",
            "           6      0.973     0.970     0.971       702\n",
            "           7      0.997     0.997     0.997       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.994     0.979     0.986       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.682     0.660     0.671       664\n",
            "          13      0.972     1.000     0.986       702\n",
            "          14      0.692     0.723     0.707       718\n",
            "\n",
            "    accuracy                          0.940     10229\n",
            "   macro avg      0.941     0.941     0.940     10229\n",
            "weighted avg      0.940     0.940     0.940     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.468     0.380     0.419       703\n",
            "           1      0.866     1.000     0.928       687\n",
            "           2      0.991     0.994     0.992       638\n",
            "           3      0.756     0.872     0.810       656\n",
            "           4      0.649     0.712     0.679       638\n",
            "           5      0.865     0.724     0.788       674\n",
            "           6      0.924     0.815     0.866       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.967     0.922     0.944       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.538     0.063     0.113       664\n",
            "          13      0.918     1.000     0.957       702\n",
            "          14      0.519     0.943     0.670       718\n",
            "\n",
            "    accuracy                          0.830     10229\n",
            "   macro avg      0.828     0.828     0.810     10229\n",
            "weighted avg      0.828     0.830     0.811     10229\n",
            "\n",
            "Done!\n",
            "Progress |███████████████-------------------------------------------------------------------------------------| 15.8% Complete\n",
            "Collecting stability analysis information...\n",
            "Iteration 0/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.987     0.980     0.984       703\n",
            "           1      0.996     1.000     0.998       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.994     0.998     0.996       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      0.999     1.000     0.999       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.994     0.997       712\n",
            "          12      0.965     0.875     0.918       664\n",
            "          13      0.999     1.000     0.999       702\n",
            "          14      0.893     0.972     0.931       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.988     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 1 iterations\n",
            "########## Outer-loop Iteration 0/1 ##########\n",
            "Initializing Trustee inner-loop with 1 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 27 and 209 leaves:\n",
            "Student model score: 0.9462252786231898\n",
            "Student model 0-0 fidelity: 0.9462252786231898\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 29 and 199 leaves:\n",
            "Student model score: 0.939294352965306\n",
            "Student model 0-1 fidelity: 0.939294352965306\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 28 and 198 leaves:\n",
            "Student model score: 0.9497262409629741\n",
            "Student model 0-2 fidelity: 0.9497262409629741\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (1.0, 0.9497262409629741)\n",
            "Top-k Prunned explanation size: 45\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.935     0.901     0.918       698\n",
            "           1      0.983     0.997     0.990       690\n",
            "           2      0.991     0.995     0.993       638\n",
            "           3      0.988     0.967     0.977       658\n",
            "           4      0.977     0.973     0.975       641\n",
            "           5      0.977     0.999     0.987       671\n",
            "           6      0.989     0.980     0.984       704\n",
            "           7      1.000     0.994     0.997       694\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.990     0.990     0.990       706\n",
            "          11      0.989     0.999     0.994       708\n",
            "          12      0.628     0.748     0.682       602\n",
            "          13      0.985     0.999     0.992       703\n",
            "          14      0.773     0.661     0.713       782\n",
            "\n",
            "    accuracy                          0.945     10229\n",
            "   macro avg      0.946     0.947     0.946     10229\n",
            "weighted avg      0.947     0.945     0.946     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.490     0.639     0.555       698\n",
            "           1      0.844     1.000     0.915       690\n",
            "           2      0.994     0.995     0.995       638\n",
            "           3      0.981     0.628     0.766       658\n",
            "           4      0.418     0.593     0.490       641\n",
            "           5      0.986     0.727     0.837       671\n",
            "           6      0.976     0.649     0.780       704\n",
            "           7      0.983     0.999     0.991       694\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.993     1.000     0.996       671\n",
            "          10      0.997     0.918     0.956       706\n",
            "          11      0.986     1.000     0.993       708\n",
            "          12      0.513     0.066     0.118       602\n",
            "          13      0.913     1.000     0.955       703\n",
            "          14      0.563     0.940     0.704       782\n",
            "\n",
            "    accuracy                          0.818     10229\n",
            "   macro avg      0.842     0.810     0.803     10229\n",
            "weighted avg      0.843     0.818     0.808     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.938     0.898     0.917       703\n",
            "           1      0.980     0.999     0.989       687\n",
            "           2      0.989     0.994     0.991       638\n",
            "           3      0.986     0.968     0.977       656\n",
            "           4      0.977     0.978     0.977       638\n",
            "           5      0.975     0.993     0.984       674\n",
            "           6      0.987     0.981     0.984       702\n",
            "           7      1.000     0.996     0.998       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.993     0.990     0.992       708\n",
            "          11      0.993     0.997     0.995       712\n",
            "          12      0.661     0.714     0.686       664\n",
            "          13      0.985     1.000     0.992       702\n",
            "          14      0.714     0.666     0.689       718\n",
            "\n",
            "    accuracy                          0.944     10229\n",
            "   macro avg      0.945     0.945     0.945     10229\n",
            "weighted avg      0.944     0.944     0.944     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.489     0.633     0.552       703\n",
            "           1      0.840     1.000     0.913       687\n",
            "           2      0.992     0.994     0.993       638\n",
            "           3      0.979     0.628     0.765       656\n",
            "           4      0.414     0.591     0.487       638\n",
            "           5      0.984     0.723     0.833       674\n",
            "           6      0.974     0.650     0.779       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.993     1.000     0.996       671\n",
            "          10      1.000     0.918     0.957       708\n",
            "          11      0.992     1.000     0.996       712\n",
            "          12      0.513     0.060     0.108       664\n",
            "          13      0.912     1.000     0.954       702\n",
            "          14      0.518     0.943     0.669       718\n",
            "\n",
            "    accuracy                          0.812     10229\n",
            "   macro avg      0.839     0.809     0.799     10229\n",
            "weighted avg      0.839     0.812     0.801     10229\n",
            "\n",
            "Progress |█████████████████████-------------------------------------------------------------------------------| 21.1% Complete\n",
            "Iteration 1/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.988     0.976     0.982       703\n",
            "           1      0.993     1.000     0.996       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.994     0.998     0.996       656\n",
            "           4      0.995     0.998     0.997       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.994     0.999     0.996       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.997     0.999       712\n",
            "          12      0.961     0.883     0.920       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.894     0.967     0.929       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.988     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 1 iterations\n",
            "########## Outer-loop Iteration 0/1 ##########\n",
            "Initializing Trustee inner-loop with 1 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 29 and 200 leaves:\n",
            "Student model score: 0.9402693972973355\n",
            "Student model 0-0 fidelity: 0.9402693972973355\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 21 and 190 leaves:\n",
            "Student model score: 0.9510731045097017\n",
            "Student model 0-1 fidelity: 0.9510731045097017\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 27 and 199 leaves:\n",
            "Student model score: 0.9567380140237992\n",
            "Student model 0-2 fidelity: 0.9567380140237992\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (1.0, 0.9567380140237992)\n",
            "Top-k Prunned explanation size: 49\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.964     0.860     0.909       694\n",
            "           1      0.975     0.996     0.985       692\n",
            "           2      0.992     0.995     0.994       637\n",
            "           3      0.986     0.985     0.986       659\n",
            "           4      0.947     0.973     0.960       640\n",
            "           5      0.949     0.999     0.973       671\n",
            "           6      0.988     0.952     0.970       705\n",
            "           7      1.000     0.996     0.998       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.985     0.992     0.988       706\n",
            "          11      0.993     0.999     0.996       710\n",
            "          12      0.676     0.718     0.696       610\n",
            "          13      0.992     1.000     0.996       702\n",
            "          14      0.748     0.727     0.737       776\n",
            "\n",
            "    accuracy                          0.945     10229\n",
            "   macro avg      0.946     0.946     0.946     10229\n",
            "weighted avg      0.946     0.945     0.945     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.520     0.702     0.598       694\n",
            "           1      0.929     0.999     0.962       692\n",
            "           2      0.992     0.995     0.994       637\n",
            "           3      0.822     0.914     0.866       659\n",
            "           4      0.700     0.586     0.638       640\n",
            "           5      0.926     0.742     0.824       671\n",
            "           6      1.000     0.765     0.867       705\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.991     0.922     0.955       706\n",
            "          11      0.990     1.000     0.995       710\n",
            "          12      0.610     0.077     0.137       610\n",
            "          13      0.909     1.000     0.953       702\n",
            "          14      0.561     0.939     0.703       776\n",
            "\n",
            "    accuracy                          0.850     10229\n",
            "   macro avg      0.862     0.843     0.832     10229\n",
            "weighted avg      0.861     0.850     0.836     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.964     0.849     0.903       703\n",
            "           1      0.967     0.996     0.981       687\n",
            "           2      0.992     0.994     0.993       638\n",
            "           3      0.982     0.985     0.983       656\n",
            "           4      0.945     0.975     0.960       638\n",
            "           5      0.948     0.993     0.970       674\n",
            "           6      0.984     0.952     0.967       702\n",
            "           7      1.000     0.996     0.998       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.987     0.992     0.989       708\n",
            "          11      0.996     0.999     0.997       712\n",
            "          12      0.711     0.694     0.703       664\n",
            "          13      0.992     1.000     0.996       702\n",
            "          14      0.704     0.740     0.721       718\n",
            "\n",
            "    accuracy                          0.943     10229\n",
            "   macro avg      0.944     0.944     0.944     10229\n",
            "weighted avg      0.944     0.943     0.943     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.521     0.694     0.595       703\n",
            "           1      0.922     0.999     0.959       687\n",
            "           2      0.992     0.994     0.993       638\n",
            "           3      0.818     0.913     0.863       656\n",
            "           4      0.696     0.585     0.635       638\n",
            "           5      0.924     0.737     0.820       674\n",
            "           6      0.998     0.766     0.867       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      0.994     0.922     0.957       708\n",
            "          11      0.993     1.000     0.997       712\n",
            "          12      0.610     0.071     0.127       664\n",
            "          13      0.909     1.000     0.953       702\n",
            "          14      0.521     0.943     0.671       718\n",
            "\n",
            "    accuracy                          0.844     10229\n",
            "   macro avg      0.858     0.842     0.828     10229\n",
            "weighted avg      0.858     0.844     0.829     10229\n",
            "\n",
            "Progress |██████████████████████████--------------------------------------------------------------------------| 26.3% Complete\n",
            "Iteration 2/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.987     0.980     0.984       703\n",
            "           1      0.994     1.000     0.997       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.995     0.998     0.997       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      0.999     1.000     0.999       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.996     0.998       708\n",
            "          11      1.000     0.994     0.997       712\n",
            "          12      0.963     0.873     0.916       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.888     0.971     0.927       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.987     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 1 iterations\n",
            "########## Outer-loop Iteration 0/1 ##########\n",
            "Initializing Trustee inner-loop with 1 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 25 and 216 leaves:\n",
            "Student model score: 0.9498442444001287\n",
            "Student model 0-0 fidelity: 0.9498442444001287\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 22 and 204 leaves:\n",
            "Student model score: 0.9393710859884287\n",
            "Student model 0-1 fidelity: 0.9393710859884287\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 21 and 195 leaves:\n",
            "Student model score: 0.938371570157877\n",
            "Student model 0-2 fidelity: 0.938371570157877\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (1.0, 0.9498442444001287)\n",
            "Top-k Prunned explanation size: 51\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.966     0.865     0.913       698\n",
            "           1      0.990     0.994     0.992       691\n",
            "           2      0.995     0.998     0.997       637\n",
            "           3      0.969     0.983     0.976       658\n",
            "           4      0.947     0.973     0.960       640\n",
            "           5      0.975     0.996     0.985       671\n",
            "           6      0.991     0.967     0.979       704\n",
            "           7      1.000     0.999     0.999       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      0.978     0.989     0.983       705\n",
            "          11      0.985     1.000     0.992       708\n",
            "          12      0.652     0.714     0.681       602\n",
            "          13      0.987     1.000     0.994       702\n",
            "          14      0.747     0.707     0.726       785\n",
            "\n",
            "    accuracy                          0.945     10229\n",
            "   macro avg      0.945     0.946     0.945     10229\n",
            "weighted avg      0.946     0.945     0.945     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.513     0.470     0.491       698\n",
            "           1      0.975     0.999     0.986       691\n",
            "           2      0.991     0.998     0.995       637\n",
            "           3      0.814     0.853     0.833       658\n",
            "           4      0.634     0.802     0.708       640\n",
            "           5      0.925     0.793     0.854       671\n",
            "           6      0.981     0.791     0.876       704\n",
            "           7      0.984     1.000     0.992       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      0.942     0.923     0.933       705\n",
            "          11      0.983     1.000     0.992       708\n",
            "          12      0.540     0.078     0.136       602\n",
            "          13      0.920     1.000     0.958       702\n",
            "          14      0.567     0.939     0.707       785\n",
            "\n",
            "    accuracy                          0.850     10229\n",
            "   macro avg      0.851     0.843     0.831     10229\n",
            "weighted avg      0.851     0.850     0.835     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.966     0.859     0.910       703\n",
            "           1      0.984     0.994     0.989       687\n",
            "           2      0.995     0.997     0.996       638\n",
            "           3      0.966     0.983     0.974       656\n",
            "           4      0.945     0.975     0.960       638\n",
            "           5      0.974     0.990     0.982       674\n",
            "           6      0.988     0.967     0.978       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      0.982     0.989     0.985       708\n",
            "          11      0.990     1.000     0.995       712\n",
            "          12      0.679     0.675     0.677       664\n",
            "          13      0.987     1.000     0.994       702\n",
            "          14      0.689     0.713     0.701       718\n",
            "\n",
            "    accuracy                          0.942     10229\n",
            "   macro avg      0.943     0.943     0.943     10229\n",
            "weighted avg      0.943     0.942     0.942     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.512     0.465     0.487       703\n",
            "           1      0.969     0.999     0.984       687\n",
            "           2      0.991     0.997     0.994       638\n",
            "           3      0.811     0.852     0.831       656\n",
            "           4      0.634     0.804     0.709       638\n",
            "           5      0.923     0.788     0.850       674\n",
            "           6      0.979     0.792     0.876       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      0.945     0.922     0.934       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.540     0.071     0.125       664\n",
            "          13      0.920     1.000     0.958       702\n",
            "          14      0.521     0.943     0.671       718\n",
            "\n",
            "    accuracy                          0.843     10229\n",
            "   macro avg      0.848     0.842     0.827     10229\n",
            "weighted avg      0.848     0.843     0.828     10229\n",
            "\n",
            "Progress |███████████████████████████████---------------------------------------------------------------------| 31.6% Complete\n",
            "Done!\n",
            "Collecting top-k prunning information...\n",
            "Iteration 1/3\n",
            "Progress |████████████████████████████████████----------------------------------------------------------------| 36.8% Complete\n",
            "Iteration 2/3\n",
            "Progress |██████████████████████████████████████████----------------------------------------------------------| 42.1% Complete\n",
            "Iteration 3/3\n",
            "Progress |███████████████████████████████████████████████-----------------------------------------------------| 47.4% Complete\n",
            "Done!\n",
            "Collecting CCP prunning information...\n",
            "Iteration 0/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.988     0.977     0.983       703\n",
            "           1      0.994     1.000     0.997       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.994     0.998     0.996       656\n",
            "           4      0.988     0.998     0.993       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.996     0.998       712\n",
            "          12      0.963     0.872     0.915       664\n",
            "          13      0.999     1.000     0.999       702\n",
            "          14      0.892     0.969     0.929       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.987     0.987     0.987     10229\n",
            "weighted avg      0.987     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 0.9999409222579974\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14172230222848653\n",
            "Student model 0-0 fidelity: 0.14172230222848653\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.075\n",
            "Student model 0-1 fidelity: 0.075\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14179930083975095\n",
            "Student model 0-2 fidelity: 0.14179930083975095\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14214677415830984\n",
            "Student model 1-0 fidelity: 0.14214677415830984\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20955946229593808\n",
            "Student model 1-1 fidelity: 0.20955946229593808\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20994871794871794\n",
            "Student model 1-2 fidelity: 0.20994871794871794\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14293640332253962\n",
            "Student model 2-0 fidelity: 0.14293640332253962\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07555555555555556\n",
            "Student model 2-1 fidelity: 0.07555555555555556\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20857838717627888\n",
            "Student model 2-2 fidelity: 0.20857838717627888\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.666590995081347, 0.20857838717627888)\n",
            "Top-k Prunned explanation size: 7\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       695\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       659\n",
            "           4      0.000     0.000     0.000       645\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.086     1.000     0.159       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.989     1.000     0.994       709\n",
            "          12      0.000     0.000     0.000       601\n",
            "          13      0.000     0.000     0.000       703\n",
            "          14      0.000     0.000     0.000       780\n",
            "\n",
            "    accuracy                          0.271     10229\n",
            "   macro avg      0.204     0.267     0.210     10229\n",
            "weighted avg      0.206     0.271     0.212     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       695\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       659\n",
            "           4      0.000     0.000     0.000       645\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.086     1.000     0.159       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.989     1.000     0.994       709\n",
            "          12      0.000     0.000     0.000       601\n",
            "          13      0.000     0.000     0.000       703\n",
            "          14      0.000     0.000     0.000       780\n",
            "\n",
            "    accuracy                          0.271     10229\n",
            "   macro avg      0.204     0.267     0.210     10229\n",
            "weighted avg      0.206     0.271     0.212     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.086     1.000     0.159       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.993     1.000     0.997       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.271     10229\n",
            "   macro avg      0.204     0.267     0.210     10229\n",
            "weighted avg      0.206     0.271     0.212     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.086     1.000     0.159       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.997     1.000     0.998       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.993     1.000     0.997       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.271     10229\n",
            "   macro avg      0.204     0.267     0.210     10229\n",
            "weighted avg      0.206     0.271     0.212     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████------------------------------------------------| 52.6% Complete\n",
            "Iteration 1/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.981     0.977     0.979       703\n",
            "           1      0.993     1.000     0.996       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.994     0.998     0.996       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.994     0.997       712\n",
            "          12      0.964     0.875     0.917       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.895     0.971     0.931       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.987     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07734911404062281\n",
            "Student model 0-0 fidelity: 0.07734911404062281\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14112846381753946\n",
            "Student model 0-1 fidelity: 0.14112846381753946\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07667324994513934\n",
            "Student model 0-2 fidelity: 0.07667324994513934\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07569721115537849\n",
            "Student model 1-0 fidelity: 0.07569721115537849\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 0 and 1 leaves:\n",
            "Student model score: 0.00924092409240924\n",
            "Student model 1-1 fidelity: 0.00924092409240924\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14206430090835037\n",
            "Student model 1-2 fidelity: 0.14206430090835037\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14238473336956084\n",
            "Student model 2-0 fidelity: 0.14238473336956084\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.2091663741657594\n",
            "Student model 2-1 fidelity: 0.2091663741657594\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 0 and 1 leaves:\n",
            "Student model score: 0.009317666460523602\n",
            "Student model 2-2 fidelity: 0.009317666460523602\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.6331413210445468, 0.14112846381753946)\n",
            "Top-k Prunned explanation size: 5\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       700\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.076     1.000     0.142       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.983     1.000     0.992       708\n",
            "          12      0.000     0.000     0.000       603\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       779\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.136     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.145     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       700\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.076     1.000     0.142       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.983     1.000     0.992       708\n",
            "          12      0.000     0.000     0.000       603\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       779\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.136     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.145     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.076     1.000     0.142       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.146     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.076     1.000     0.142       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.146     10229\n",
            "\n",
            "Progress |█████████████████████████████████████████████████████████-------------------------------------------| 57.9% Complete\n",
            "Iteration 2/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.983     0.979     0.981       703\n",
            "           1      0.994     1.000     0.997       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.995     0.998     0.997       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      0.999     0.996     0.997       712\n",
            "          12      0.965     0.875     0.918       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.892     0.971     0.930       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.988     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 0 and 1 leaves:\n",
            "Student model score: 0.00744925716677129\n",
            "Student model 0-0 fidelity: 0.00744925716677129\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14150722785540928\n",
            "Student model 0-1 fidelity: 0.14150722785540928\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07630524523680236\n",
            "Student model 0-2 fidelity: 0.07630524523680236\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 0 and 1 leaves:\n",
            "Student model score: 0.00870105655686762\n",
            "Student model 1-0 fidelity: 0.00870105655686762\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07640362095338399\n",
            "Student model 1-1 fidelity: 0.07640362095338399\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07390521623079763\n",
            "Student model 1-2 fidelity: 0.07390521623079763\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20771759508474724\n",
            "Student model 2-0 fidelity: 0.20771759508474724\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07617109634551494\n",
            "Student model 2-1 fidelity: 0.07617109634551494\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.21090224290169982\n",
            "Student model 2-2 fidelity: 0.21090224290169982\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.5499999999999999, 0.14150722785540928)\n",
            "Top-k Prunned explanation size: 5\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       700\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       637\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       640\n",
            "           5      0.076     1.000     0.142       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.986     1.000     0.993       710\n",
            "          12      0.000     0.000     0.000       602\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       781\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.136     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.145     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       700\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       637\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       640\n",
            "           5      0.076     1.000     0.142       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.986     1.000     0.993       710\n",
            "          12      0.000     0.000     0.000       602\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       781\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.136     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.145     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.077     1.000     0.142       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.146     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.077     1.000     0.142       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.140     0.203     0.146     10229\n",
            "\n",
            "Progress |███████████████████████████████████████████████████████████████-------------------------------------| 63.2% Complete\n",
            "Done!\n",
            "Collecting max depth prunning information...\n",
            "Iteration 1/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.986     0.977     0.981       703\n",
            "           1      0.993     1.000     0.996       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.989     0.997     0.993       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      0.999     1.000     0.999       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.996     0.998       708\n",
            "          11      0.999     0.996     0.997       712\n",
            "          12      0.967     0.873     0.918       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.894     0.974     0.932       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.987     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07556142668428005\n",
            "Student model 0-0 fidelity: 0.07556142668428005\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07558498896247241\n",
            "Student model 0-1 fidelity: 0.07558498896247241\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07539682539682539\n",
            "Student model 0-2 fidelity: 0.07539682539682539\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07558356272999613\n",
            "Student model 1-0 fidelity: 0.07558356272999613\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.04944213204565555\n",
            "Student model 1-1 fidelity: 0.04944213204565555\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07532931457914713\n",
            "Student model 1-2 fidelity: 0.07532931457914713\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.046581888246628136\n",
            "Student model 2-0 fidelity: 0.046581888246628136\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07412137438643462\n",
            "Student model 2-1 fidelity: 0.07412137438643462\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07745415318230853\n",
            "Student model 2-2 fidelity: 0.07745415318230853\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.44305080451032564, 0.07558498896247241)\n",
            "Top-k Prunned explanation size: 3\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.073     1.000     0.136       697\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.000     0.000     0.000       637\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       643\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.000     0.000     0.000       694\n",
            "           8      0.993     1.000     0.996       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       705\n",
            "          11      0.000     0.000     0.000       710\n",
            "          12      0.000     0.000     0.000       600\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       782\n",
            "\n",
            "    accuracy                          0.133     10229\n",
            "   macro avg      0.071     0.133     0.075     10229\n",
            "weighted avg      0.069     0.133     0.074     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.073     1.000     0.136       697\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.000     0.000     0.000       637\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       643\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.000     0.000     0.000       694\n",
            "           8      0.993     1.000     0.996       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       705\n",
            "          11      0.000     0.000     0.000       710\n",
            "          12      0.000     0.000     0.000       600\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       782\n",
            "\n",
            "    accuracy                          0.133     10229\n",
            "   macro avg      0.071     0.133     0.075     10229\n",
            "weighted avg      0.069     0.133     0.074     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.074     1.000     0.137       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.000     0.000     0.000       693\n",
            "           8      0.993     1.000     0.996       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.134     10229\n",
            "   macro avg      0.071     0.133     0.076     10229\n",
            "weighted avg      0.069     0.134     0.074     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.074     1.000     0.137       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.000     0.000     0.000       693\n",
            "           8      0.993     1.000     0.996       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.134     10229\n",
            "   macro avg      0.071     0.133     0.076     10229\n",
            "weighted avg      0.069     0.134     0.074     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████████████████████--------------------------------| 68.4% Complete\n",
            "Iteration 2/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.989     0.980     0.984       703\n",
            "           1      0.996     1.000     0.998       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.997     0.998     0.998       656\n",
            "           4      0.994     1.000     0.997       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.996     0.998       712\n",
            "          12      0.970     0.870     0.917       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.887     0.976     0.930       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.988     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.2103852329154572\n",
            "Student model 0-0 fidelity: 0.2103852329154572\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.1457586837213634\n",
            "Student model 0-1 fidelity: 0.1457586837213634\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.20979501726310518\n",
            "Student model 0-2 fidelity: 0.20979501726310518\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.11768754529231269\n",
            "Student model 1-0 fidelity: 0.11768754529231269\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.14195180729329474\n",
            "Student model 1-1 fidelity: 0.14195180729329474\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.14432668632546006\n",
            "Student model 1-2 fidelity: 0.14432668632546006\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14146876635103886\n",
            "Student model 2-0 fidelity: 0.14146876635103886\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 2 and 4 leaves:\n",
            "Student model score: 0.14357823700774225\n",
            "Student model 2-1 fidelity: 0.14357823700774225\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14200715625983654\n",
            "Student model 2-2 fidelity: 0.14200715625983654\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.7465045677361853, 0.14432668632546006)\n",
            "Top-k Prunned explanation size: 5\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       697\n",
            "           1      0.000     0.000     0.000       690\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       657\n",
            "           4      0.000     0.000     0.000       642\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.997     1.000     0.999       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      1.000     0.014     0.028       706\n",
            "          11      0.000     0.000     0.000       709\n",
            "          12      0.000     0.000     0.000       596\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.089     1.000     0.164       790\n",
            "\n",
            "    accuracy                          0.211     10229\n",
            "   macro avg      0.206     0.201     0.146     10229\n",
            "weighted avg      0.208     0.211     0.147     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       697\n",
            "           1      0.000     0.000     0.000       690\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       657\n",
            "           4      0.000     0.000     0.000       642\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.000     0.000     0.000       709\n",
            "          12      0.000     0.000     0.000       596\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.089     1.000     0.164       790\n",
            "\n",
            "    accuracy                          0.210     10229\n",
            "   macro avg      0.138     0.200     0.144     10229\n",
            "weighted avg      0.138     0.210     0.145     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.997     1.000     0.999       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      1.000     0.014     0.028       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.081     1.000     0.150       718\n",
            "\n",
            "    accuracy                          0.204     10229\n",
            "   macro avg      0.205     0.201     0.145     10229\n",
            "weighted avg      0.207     0.204     0.145     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.081     1.000     0.150       718\n",
            "\n",
            "    accuracy                          0.203     10229\n",
            "   macro avg      0.138     0.200     0.143     10229\n",
            "weighted avg      0.137     0.203     0.142     10229\n",
            "\n",
            "Progress |█████████████████████████████████████████████████████████████████████████---------------------------| 73.7% Complete\n",
            "Iteration 3/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.987     0.980     0.984       703\n",
            "           1      0.996     1.000     0.998       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.994     0.998     0.996       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      0.999     1.000     0.999       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      1.000     0.994     0.997       712\n",
            "          12      0.960     0.873     0.915       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.890     0.968     0.927       718\n",
            "\n",
            "    accuracy                          0.987     10229\n",
            "   macro avg      0.988     0.987     0.987     10229\n",
            "weighted avg      0.987     0.987     0.987     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 3 and 5 leaves:\n",
            "Student model score: 0.18012961531601165\n",
            "Student model 0-0 fidelity: 0.18012961531601165\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 3 and 6 leaves:\n",
            "Student model score: 0.21179159781952217\n",
            "Student model 0-1 fidelity: 0.21179159781952217\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 3 and 5 leaves:\n",
            "Student model score: 0.2130740157316572\n",
            "Student model 0-2 fidelity: 0.2130740157316572\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 3 and 7 leaves:\n",
            "Student model score: 0.2527813939340792\n",
            "Student model 1-0 fidelity: 0.2527813939340792\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 3 and 6 leaves:\n",
            "Student model score: 0.20992380615760778\n",
            "Student model 1-1 fidelity: 0.20992380615760778\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 3 and 6 leaves:\n",
            "Student model score: 0.18796334923080765\n",
            "Student model 1-2 fidelity: 0.18796334923080765\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 3 and 7 leaves:\n",
            "Student model score: 0.2140387702513607\n",
            "Student model 2-0 fidelity: 0.2140387702513607\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 3 and 5 leaves:\n",
            "Student model score: 0.17709946227391063\n",
            "Student model 2-1 fidelity: 0.17709946227391063\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 3 and 7 leaves:\n",
            "Student model score: 0.21290121810260515\n",
            "Student model 2-2 fidelity: 0.21290121810260515\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.6997560366255077, 0.2140387702513607)\n",
            "Top-k Prunned explanation size: 7\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.001     0.003       698\n",
            "           1      0.000     0.000     0.000       690\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.997     0.996     0.996       694\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.833     0.014     0.028       706\n",
            "          11      0.985     1.000     0.992       708\n",
            "          12      0.074     1.000     0.138       604\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       781\n",
            "\n",
            "    accuracy                          0.262     10229\n",
            "   macro avg      0.326     0.267     0.210     10229\n",
            "weighted avg      0.331     0.262     0.211     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       698\n",
            "           1      0.000     0.000     0.000       690\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     0.999     0.991       694\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.983     1.000     0.992       708\n",
            "          12      0.074     1.000     0.138       604\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       781\n",
            "\n",
            "    accuracy                          0.261     10229\n",
            "   macro avg      0.203     0.267     0.208     10229\n",
            "weighted avg      0.204     0.261     0.209     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.001     0.003       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.997     0.997     0.997       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.833     0.014     0.028       708\n",
            "          11      0.990     1.000     0.995       712\n",
            "          12      0.082     1.000     0.151       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.268     10229\n",
            "   macro avg      0.327     0.268     0.212     10229\n",
            "weighted avg      0.333     0.268     0.213     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.082     1.000     0.151       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.267     10229\n",
            "   macro avg      0.203     0.267     0.209     10229\n",
            "weighted avg      0.205     0.267     0.211     10229\n",
            "\n",
            "Progress |██████████████████████████████████████████████████████████████████████████████----------------------| 78.9% Complete\n",
            "Done!\n",
            "Collecting max leaves prunning information...\n",
            "Iteration 2/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.986     0.974     0.980       703\n",
            "           1      0.991     1.000     0.996       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.994     0.998     0.996       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      0.999     0.994     0.996       712\n",
            "          12      0.964     0.889     0.925       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.902     0.971     0.935       718\n",
            "\n",
            "    accuracy                          0.988     10229\n",
            "   macro avg      0.988     0.988     0.988     10229\n",
            "weighted avg      0.988     0.988     0.988     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07474783461771804\n",
            "Student model 0-0 fidelity: 0.07474783461771804\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.05546895421296998\n",
            "Student model 0-1 fidelity: 0.05546895421296998\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.0734649914783347\n",
            "Student model 0-2 fidelity: 0.0734649914783347\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07514910536779325\n",
            "Student model 1-0 fidelity: 0.07514910536779325\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07620941020543406\n",
            "Student model 1-1 fidelity: 0.07620941020543406\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.075636875214823\n",
            "Student model 1-2 fidelity: 0.075636875214823\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.035583365750971846\n",
            "Student model 2-0 fidelity: 0.035583365750971846\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.0750740113505617\n",
            "Student model 2-1 fidelity: 0.0750740113505617\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 1 and 2 leaves:\n",
            "Student model score: 0.07480198846433177\n",
            "Student model 2-2 fidelity: 0.07480198846433177\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.5555555555555555, 0.07474783461771804)\n",
            "Top-k Prunned explanation size: 3\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       695\n",
            "           1      0.000     0.000     0.000       693\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.070     1.000     0.132       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.000     0.000     0.000       709\n",
            "          12      0.000     0.000     0.000       612\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       773\n",
            "\n",
            "    accuracy                          0.133     10229\n",
            "   macro avg      0.070     0.133     0.075     10229\n",
            "weighted avg      0.071     0.133     0.076     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       695\n",
            "           1      0.000     0.000     0.000       693\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.070     1.000     0.132       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.000     0.000     0.000       709\n",
            "          12      0.000     0.000     0.000       612\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       773\n",
            "\n",
            "    accuracy                          0.133     10229\n",
            "   macro avg      0.070     0.133     0.075     10229\n",
            "weighted avg      0.071     0.133     0.076     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.071     1.000     0.132       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.134     10229\n",
            "   macro avg      0.070     0.133     0.075     10229\n",
            "weighted avg      0.071     0.134     0.076     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.071     1.000     0.132       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.000     0.000     0.000       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.134     10229\n",
            "   macro avg      0.070     0.133     0.075     10229\n",
            "weighted avg      0.071     0.134     0.076     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████████████████████████████████████----------------| 84.2% Complete\n",
            "Iteration 3/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.993     0.979     0.986       703\n",
            "           1      0.993     1.000     0.996       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.995     0.998     0.997       656\n",
            "           4      0.989     1.000     0.995       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.997     0.999     0.998       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     1.000     1.000       708\n",
            "          11      0.999     0.996     0.997       712\n",
            "          12      0.965     0.877     0.919       664\n",
            "          13      0.999     1.000     0.999       702\n",
            "          14      0.897     0.972     0.933       718\n",
            "\n",
            "    accuracy                          0.988     10229\n",
            "   macro avg      0.988     0.988     0.988     10229\n",
            "weighted avg      0.988     0.988     0.988     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14323062558356675\n",
            "Student model 0-0 fidelity: 0.14323062558356675\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14347108804551656\n",
            "Student model 0-1 fidelity: 0.14347108804551656\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.12067277179098265\n",
            "Student model 0-2 fidelity: 0.12067277179098265\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14354426061743136\n",
            "Student model 1-0 fidelity: 0.14354426061743136\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.1430043667285007\n",
            "Student model 1-1 fidelity: 0.1430043667285007\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14181401226069598\n",
            "Student model 1-2 fidelity: 0.14181401226069598\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.14133123595196254\n",
            "Student model 2-0 fidelity: 0.14133123595196254\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.12315463879106339\n",
            "Student model 2-1 fidelity: 0.12315463879106339\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 2 and 3 leaves:\n",
            "Student model score: 0.12343664917689907\n",
            "Student model 2-2 fidelity: 0.12343664917689907\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.6601473922902494, 0.14347108804551656)\n",
            "Top-k Prunned explanation size: 5\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       693\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.072     1.000     0.134       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       645\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       703\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       710\n",
            "          12      0.000     0.000     0.000       603\n",
            "          13      0.000     0.000     0.000       703\n",
            "          14      0.000     0.000     0.000       778\n",
            "\n",
            "    accuracy                          0.195     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.136     0.195     0.140     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       693\n",
            "           1      0.000     0.000     0.000       692\n",
            "           2      0.072     1.000     0.134       638\n",
            "           3      0.000     0.000     0.000       658\n",
            "           4      0.000     0.000     0.000       645\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       703\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       710\n",
            "          12      0.000     0.000     0.000       603\n",
            "          13      0.000     0.000     0.000       703\n",
            "          14      0.000     0.000     0.000       778\n",
            "\n",
            "    accuracy                          0.195     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.136     0.195     0.140     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.072     1.000     0.134       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.195     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.136     0.195     0.140     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.072     1.000     0.134       638\n",
            "           3      0.000     0.000     0.000       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.000     0.000     0.000       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.195     10229\n",
            "   macro avg      0.137     0.200     0.142     10229\n",
            "weighted avg      0.136     0.195     0.140     10229\n",
            "\n",
            "Progress |█████████████████████████████████████████████████████████████████████████████████████████-----------| 89.5% Complete\n",
            "Iteration 4/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.983     0.976     0.979       703\n",
            "           1      0.994     1.000     0.997       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.994     0.998     0.996       656\n",
            "           4      0.994     0.998     0.996       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.996     0.999     0.997       702\n",
            "           7      1.000     1.000     1.000       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      1.000     1.000     1.000       671\n",
            "          10      1.000     0.997     0.999       708\n",
            "          11      0.999     0.996     0.997       712\n",
            "          12      0.967     0.884     0.924       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.901     0.972     0.935       718\n",
            "\n",
            "    accuracy                          0.988     10229\n",
            "   macro avg      0.988     0.988     0.988     10229\n",
            "weighted avg      0.988     0.988     0.988     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 1.0\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.18379242784201277\n",
            "Student model 0-0 fidelity: 0.18379242784201277\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20904423849657114\n",
            "Student model 0-1 fidelity: 0.20904423849657114\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.208267899412178\n",
            "Student model 0-2 fidelity: 0.208267899412178\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.21011561835804404\n",
            "Student model 1-0 fidelity: 0.21011561835804404\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20953815261044176\n",
            "Student model 1-1 fidelity: 0.20953815261044176\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20845993934832982\n",
            "Student model 1-2 fidelity: 0.20845993934832982\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.17606200484831014\n",
            "Student model 2-0 fidelity: 0.17606200484831014\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.20814495685946185\n",
            "Student model 2-1 fidelity: 0.20814495685946185\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 3 and 4 leaves:\n",
            "Student model score: 0.1747135719563134\n",
            "Student model 2-2 fidelity: 0.1747135719563134\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.8664680569347899, 0.21011561835804404)\n",
            "Top-k Prunned explanation size: 7\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       698\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.081     1.000     0.150       659\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.986     1.000     0.993       710\n",
            "          12      0.000     0.000     0.000       607\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       775\n",
            "\n",
            "    accuracy                          0.266     10229\n",
            "   macro avg      0.203     0.267     0.209     10229\n",
            "weighted avg      0.205     0.266     0.211     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       698\n",
            "           1      0.000     0.000     0.000       691\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.081     1.000     0.150       659\n",
            "           4      0.000     0.000     0.000       641\n",
            "           5      0.000     0.000     0.000       671\n",
            "           6      0.000     0.000     0.000       704\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       706\n",
            "          11      0.986     1.000     0.993       710\n",
            "          12      0.000     0.000     0.000       607\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       775\n",
            "\n",
            "    accuracy                          0.266     10229\n",
            "   macro avg      0.203     0.267     0.209     10229\n",
            "weighted avg      0.205     0.266     0.211     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.081     1.000     0.149       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.266     10229\n",
            "   macro avg      0.203     0.267     0.209     10229\n",
            "weighted avg      0.205     0.266     0.211     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       703\n",
            "           1      0.000     0.000     0.000       687\n",
            "           2      0.000     0.000     0.000       638\n",
            "           3      0.081     1.000     0.149       656\n",
            "           4      0.000     0.000     0.000       638\n",
            "           5      0.000     0.000     0.000       674\n",
            "           6      0.000     0.000     0.000       702\n",
            "           7      0.983     1.000     0.991       693\n",
            "           8      0.998     1.000     0.999       663\n",
            "           9      0.000     0.000     0.000       671\n",
            "          10      0.000     0.000     0.000       708\n",
            "          11      0.989     1.000     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.000     0.000     0.000       702\n",
            "          14      0.000     0.000     0.000       718\n",
            "\n",
            "    accuracy                          0.266     10229\n",
            "   macro avg      0.203     0.267     0.209     10229\n",
            "weighted avg      0.205     0.266     0.211     10229\n",
            "\n",
            "Progress |██████████████████████████████████████████████████████████████████████████████████████████████------| 94.7% Complete\n",
            "Done!\n",
            "Collecting features prunning information...\n",
            "Iteration 1/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.981     0.962     0.971       703\n",
            "           1      0.993     0.994     0.993       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.992     0.998     0.995       656\n",
            "           4      0.989     0.998     0.994       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.994     0.999     0.996       702\n",
            "           7      0.997     0.999     0.998       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      1.000     1.000     1.000       708\n",
            "          11      0.997     0.993     0.995       712\n",
            "          12      0.957     0.875     0.914       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.893     0.969     0.930       718\n",
            "\n",
            "    accuracy                          0.985     10229\n",
            "   macro avg      0.986     0.985     0.985     10229\n",
            "weighted avg      0.986     0.985     0.985     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 0.9999400205557083\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 24 and 214 leaves:\n",
            "Student model score: 0.9415847550035085\n",
            "Student model 0-0 fidelity: 0.9415847550035085\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 28 and 209 leaves:\n",
            "Student model score: 0.9384353570381619\n",
            "Student model 0-1 fidelity: 0.9384353570381619\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 23 and 227 leaves:\n",
            "Student model score: 0.9378247177042526\n",
            "Student model 0-2 fidelity: 0.9378247177042526\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 23 and 223 leaves:\n",
            "Student model score: 0.9498145753347014\n",
            "Student model 1-0 fidelity: 0.9498145753347014\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 19 and 215 leaves:\n",
            "Student model score: 0.9343832157143966\n",
            "Student model 1-1 fidelity: 0.9343832157143966\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 22 and 218 leaves:\n",
            "Student model score: 0.9384022606435692\n",
            "Student model 1-2 fidelity: 0.9384022606435692\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 21 and 219 leaves:\n",
            "Student model score: 0.9453854367899694\n",
            "Student model 2-0 fidelity: 0.9453854367899694\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 20 and 203 leaves:\n",
            "Student model score: 0.9403819993949583\n",
            "Student model 2-1 fidelity: 0.9403819993949583\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 27 and 219 leaves:\n",
            "Student model score: 0.9421753612174392\n",
            "Student model 2-2 fidelity: 0.9421753612174392\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.9284338945806664, 0.9415847550035085)\n",
            "Top-k Prunned explanation size: 43\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.937     0.865     0.900       689\n",
            "           1      0.970     0.996     0.983       688\n",
            "           2      0.998     0.995     0.997       637\n",
            "           3      0.979     0.991     0.985       660\n",
            "           4      0.992     0.966     0.979       644\n",
            "           5      0.987     0.993     0.990       671\n",
            "           6      0.994     0.984     0.989       705\n",
            "           7      0.987     0.999     0.993       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.982     1.000     0.991       673\n",
            "          10      0.994     0.989     0.992       708\n",
            "          11      0.987     0.994     0.991       709\n",
            "          12      0.607     0.728     0.662       607\n",
            "          13      0.992     1.000     0.996       702\n",
            "          14      0.747     0.652     0.696       779\n",
            "\n",
            "    accuracy                          0.942     10229\n",
            "   macro avg      0.944     0.943     0.943     10229\n",
            "weighted avg      0.944     0.942     0.942     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.332     0.350     0.341       689\n",
            "           1      0.959     0.683     0.798       688\n",
            "           2      0.647     1.000     0.786       637\n",
            "           3      0.916     0.838     0.875       660\n",
            "           4      1.000     0.255     0.406       644\n",
            "           5      0.688     0.902     0.781       671\n",
            "           6      0.998     0.800     0.888       705\n",
            "           7      0.984     0.805     0.886       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.931     0.826     0.876       673\n",
            "          10      0.561     0.989     0.716       708\n",
            "          11      0.972     0.994     0.983       709\n",
            "          12      0.000     0.000     0.000       607\n",
            "          13      0.915     1.000     0.956       702\n",
            "          14      0.577     0.927     0.711       779\n",
            "\n",
            "    accuracy                          0.767     10229\n",
            "   macro avg      0.766     0.758     0.733     10229\n",
            "weighted avg      0.769     0.767     0.740     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.931     0.842     0.884       703\n",
            "           1      0.965     0.991     0.978       687\n",
            "           2      0.998     0.994     0.996       638\n",
            "           3      0.973     0.991     0.982       656\n",
            "           4      0.989     0.972     0.980       638\n",
            "           5      0.985     0.987     0.986       674\n",
            "           6      0.991     0.986     0.989       702\n",
            "           7      0.986     0.999     0.992       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.980     1.000     0.990       671\n",
            "          10      0.994     0.989     0.992       708\n",
            "          11      0.989     0.992     0.990       712\n",
            "          12      0.633     0.694     0.662       664\n",
            "          13      0.992     1.000     0.996       702\n",
            "          14      0.693     0.656     0.674       718\n",
            "\n",
            "    accuracy                          0.939     10229\n",
            "   macro avg      0.940     0.939     0.939     10229\n",
            "weighted avg      0.939     0.939     0.939     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.343     0.354     0.349       703\n",
            "           1      0.957     0.683     0.797       687\n",
            "           2      0.648     1.000     0.787       638\n",
            "           3      0.911     0.838     0.873       656\n",
            "           4      1.000     0.257     0.409       638\n",
            "           5      0.687     0.896     0.778       674\n",
            "           6      0.996     0.802     0.889       702\n",
            "           7      0.984     0.807     0.887       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.931     0.829     0.877       671\n",
            "          10      0.561     0.989     0.716       708\n",
            "          11      0.974     0.992     0.983       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.915     1.000     0.956       702\n",
            "          14      0.532     0.926     0.675       718\n",
            "\n",
            "    accuracy                          0.761     10229\n",
            "   macro avg      0.763     0.758     0.732     10229\n",
            "weighted avg      0.762     0.761     0.734     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
            "Iteration 2/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.981     0.933     0.956       703\n",
            "           1      0.983     0.994     0.988       687\n",
            "           2      0.998     0.998     0.998       638\n",
            "           3      0.989     0.998     0.994       656\n",
            "           4      0.992     0.997     0.995       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.993     0.999     0.996       702\n",
            "           7      0.997     0.999     0.998       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      1.000     1.000     1.000       708\n",
            "          11      0.989     0.993     0.991       712\n",
            "          12      0.951     0.869     0.908       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.887     0.969     0.926       718\n",
            "\n",
            "    accuracy                          0.983     10229\n",
            "   macro avg      0.984     0.983     0.983     10229\n",
            "weighted avg      0.983     0.983     0.983     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 0.9998179208813033\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 22 and 236 leaves:\n",
            "Student model score: 0.9338503200917322\n",
            "Student model 0-0 fidelity: 0.9338503200917322\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 25 and 230 leaves:\n",
            "Student model score: 0.9457361458693594\n",
            "Student model 0-1 fidelity: 0.9457361458693594\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 23 and 230 leaves:\n",
            "Student model score: 0.9281575212281541\n",
            "Student model 0-2 fidelity: 0.9281575212281541\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 26 and 225 leaves:\n",
            "Student model score: 0.9377881531675651\n",
            "Student model 1-0 fidelity: 0.9377881531675651\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 20 and 220 leaves:\n",
            "Student model score: 0.9502502840569964\n",
            "Student model 1-1 fidelity: 0.9502502840569964\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 29 and 214 leaves:\n",
            "Student model score: 0.959297114067595\n",
            "Student model 1-2 fidelity: 0.959297114067595\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 24 and 231 leaves:\n",
            "Student model score: 0.9252757875463582\n",
            "Student model 2-0 fidelity: 0.9252757875463582\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 22 and 217 leaves:\n",
            "Student model score: 0.9364640023065759\n",
            "Student model 2-1 fidelity: 0.9364640023065759\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 28 and 217 leaves:\n",
            "Student model score: 0.9518091057030579\n",
            "Student model 2-2 fidelity: 0.9518091057030579\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.8213493157550409, 0.959297114067595)\n",
            "Top-k Prunned explanation size: 49\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.917     0.837     0.875       669\n",
            "           1      0.966     0.974     0.970       695\n",
            "           2      0.995     0.997     0.996       638\n",
            "           3      0.974     0.950     0.962       662\n",
            "           4      0.972     0.984     0.978       641\n",
            "           5      0.963     1.000     0.981       671\n",
            "           6      0.981     0.976     0.979       706\n",
            "           7      0.994     0.997     0.996       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.978     1.000     0.989       673\n",
            "          10      0.994     0.994     0.994       708\n",
            "          11      0.985     0.987     0.986       715\n",
            "          12      0.613     0.761     0.679       607\n",
            "          13      0.987     1.000     0.994       702\n",
            "          14      0.762     0.628     0.689       785\n",
            "\n",
            "    accuracy                          0.937     10229\n",
            "   macro avg      0.939     0.939     0.938     10229\n",
            "weighted avg      0.939     0.937     0.937     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.498     0.616     0.550       669\n",
            "           1      0.918     0.243     0.385       695\n",
            "           2      0.995     0.997     0.996       638\n",
            "           3      0.952     0.817     0.880       662\n",
            "           4      0.452     0.928     0.608       641\n",
            "           5      0.905     0.794     0.846       671\n",
            "           6      0.950     0.836     0.889       706\n",
            "           7      0.997     0.993     0.995       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.957     0.826     0.887       673\n",
            "          10      0.994     0.994     0.994       708\n",
            "          11      0.994     0.986     0.990       715\n",
            "          12      0.000     0.000     0.000       607\n",
            "          13      0.782     1.000     0.878       702\n",
            "          14      0.586     0.922     0.717       785\n",
            "\n",
            "    accuracy                          0.803     10229\n",
            "   macro avg      0.799     0.797     0.774     10229\n",
            "weighted avg      0.804     0.803     0.780     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.948     0.824     0.881       703\n",
            "           1      0.961     0.981     0.971       687\n",
            "           2      0.994     0.995     0.995       638\n",
            "           3      0.967     0.953     0.960       656\n",
            "           4      0.974     0.991     0.982       638\n",
            "           5      0.963     0.996     0.979       674\n",
            "           6      0.976     0.976     0.976       702\n",
            "           7      0.993     0.997     0.995       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.975     1.000     0.987       671\n",
            "          10      0.994     0.994     0.994       708\n",
            "          11      0.986     0.993     0.990       712\n",
            "          12      0.645     0.732     0.685       664\n",
            "          13      0.987     1.000     0.994       702\n",
            "          14      0.706     0.636     0.670       718\n",
            "\n",
            "    accuracy                          0.937     10229\n",
            "   macro avg      0.938     0.938     0.937     10229\n",
            "weighted avg      0.938     0.937     0.937     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.505     0.595     0.546       703\n",
            "           1      0.913     0.245     0.386       687\n",
            "           2      0.994     0.995     0.995       638\n",
            "           3      0.947     0.820     0.879       656\n",
            "           4      0.450     0.928     0.606       638\n",
            "           5      0.903     0.789     0.842       674\n",
            "           6      0.948     0.839     0.890       702\n",
            "           7      0.996     0.993     0.994       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.957     0.829     0.888       671\n",
            "          10      0.994     0.994     0.994       708\n",
            "          11      0.996     0.992     0.994       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.782     1.000     0.878       702\n",
            "          14      0.538     0.926     0.681       718\n",
            "\n",
            "    accuracy                          0.797     10229\n",
            "   macro avg      0.795     0.796     0.772     10229\n",
            "weighted avg      0.796     0.797     0.773     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
            "Iteration 3/3\n",
            "Fitting blackbox model...\n",
            "Done!\n",
            "Blackbox model score report with training data:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.982     0.933     0.957       703\n",
            "           1      0.983     0.994     0.988       687\n",
            "           2      1.000     0.998     0.999       638\n",
            "           3      0.991     0.998     0.995       656\n",
            "           4      0.989     0.997     0.993       638\n",
            "           5      0.999     0.994     0.996       674\n",
            "           6      0.993     0.999     0.996       702\n",
            "           7      0.997     0.999     0.998       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.997     1.000     0.999       671\n",
            "          10      1.000     1.000     1.000       708\n",
            "          11      0.987     0.993     0.990       712\n",
            "          12      0.954     0.866     0.908       664\n",
            "          13      1.000     1.000     1.000       702\n",
            "          14      0.885     0.972     0.926       718\n",
            "\n",
            "    accuracy                          0.983     10229\n",
            "   macro avg      0.984     0.983     0.983     10229\n",
            "weighted avg      0.983     0.983     0.983     10229\n",
            "\n",
            "Using Classification Trustee algorithm to extract DT...\n",
            "Initializing training dataset using RandomForestClassifier(n_jobs=4) as expert model\n",
            "Expert model score: 0.9998218832395825\n",
            "Initializing Trustee outer-loop with 3 iterations\n",
            "########## Outer-loop Iteration 0/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (16706, 16706) entries\n",
            "Student model 0-0 trained with depth 27 and 219 leaves:\n",
            "Student model score: 0.930022679356282\n",
            "Student model 0-0 fidelity: 0.930022679356282\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (18210, 18210) entries\n",
            "Student model 0-1 trained with depth 28 and 214 leaves:\n",
            "Student model score: 0.9354529772490365\n",
            "Student model 0-1 fidelity: 0.9354529772490365\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (19714, 19714) entries\n",
            "Student model 0-2 trained with depth 30 and 220 leaves:\n",
            "Student model score: 0.9426250918341317\n",
            "Student model 0-2 fidelity: 0.9426250918341317\n",
            "########## Outer-loop Iteration 1/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (21218, 21218) entries\n",
            "Student model 1-0 trained with depth 19 and 218 leaves:\n",
            "Student model score: 0.9425779602727711\n",
            "Student model 1-0 fidelity: 0.9425779602727711\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (22722, 22722) entries\n",
            "Student model 1-1 trained with depth 23 and 232 leaves:\n",
            "Student model score: 0.9470041308669453\n",
            "Student model 1-1 fidelity: 0.9470041308669453\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (24226, 24226) entries\n",
            "Student model 1-2 trained with depth 23 and 221 leaves:\n",
            "Student model score: 0.9458913211430654\n",
            "Student model 1-2 fidelity: 0.9458913211430654\n",
            "########## Outer-loop Iteration 2/3 ##########\n",
            "Initializing Trustee inner-loop with 3 iterations\n",
            "########## Inner-loop Iteration 0/3 ##########\n",
            "Sampling 5011 points from training dataset with (25730, 25730) entries\n",
            "Student model 2-0 trained with depth 21 and 229 leaves:\n",
            "Student model score: 0.9373861153191475\n",
            "Student model 2-0 fidelity: 0.9373861153191475\n",
            "########## Inner-loop Iteration 1/3 ##########\n",
            "Sampling 5011 points from training dataset with (27234, 27234) entries\n",
            "Student model 2-1 trained with depth 20 and 223 leaves:\n",
            "Student model score: 0.9409818230980732\n",
            "Student model 2-1 fidelity: 0.9409818230980732\n",
            "########## Inner-loop Iteration 2/3 ##########\n",
            "Sampling 5011 points from training dataset with (28738, 28738) entries\n",
            "Student model 2-2 trained with depth 24 and 219 leaves:\n",
            "Student model score: 0.9461385716703506\n",
            "Student model 2-2 fidelity: 0.9461385716703506\n",
            "Done!\n",
            "Model explanation training (agreement, fidelity): (0.869868845266851, 0.9426250918341317)\n",
            "Top-k Prunned explanation size: 49\n",
            "Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.902     0.841     0.871       668\n",
            "           1      0.977     0.977     0.977       695\n",
            "           2      0.975     0.998     0.987       637\n",
            "           3      0.975     0.986     0.980       661\n",
            "           4      0.970     0.970     0.970       643\n",
            "           5      0.996     1.000     0.998       671\n",
            "           6      0.987     0.970     0.979       706\n",
            "           7      0.994     0.990     0.992       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.978     0.997     0.987       673\n",
            "          10      0.987     0.989     0.988       708\n",
            "          11      0.993     0.983     0.988       716\n",
            "          12      0.657     0.695     0.675       603\n",
            "          13      0.982     1.000     0.991       702\n",
            "          14      0.749     0.729     0.739       789\n",
            "\n",
            "    accuracy                          0.941     10229\n",
            "   macro avg      0.941     0.942     0.941     10229\n",
            "weighted avg      0.942     0.941     0.941     10229\n",
            "\n",
            "Top-k Model explanation global fidelity report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.264     0.392     0.315       668\n",
            "           1      0.949     0.594     0.731       695\n",
            "           2      0.894     0.975     0.932       637\n",
            "           3      0.916     0.838     0.875       661\n",
            "           4      0.894     0.684     0.775       643\n",
            "           5      0.948     0.729     0.824       671\n",
            "           6      0.898     0.840     0.868       706\n",
            "           7      0.565     0.990     0.720       694\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.961     0.877     0.917       673\n",
            "          10      0.942     0.527     0.676       708\n",
            "          11      0.769     0.993     0.867       716\n",
            "          12      0.000     0.000     0.000       603\n",
            "          13      0.743     0.813     0.777       702\n",
            "          14      0.582     0.924     0.714       789\n",
            "\n",
            "    accuracy                          0.752     10229\n",
            "   macro avg      0.755     0.745     0.733     10229\n",
            "weighted avg      0.758     0.752     0.737     10229\n",
            "\n",
            "Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.923     0.818     0.867       703\n",
            "           1      0.971     0.983     0.977       687\n",
            "           2      0.975     0.997     0.986       638\n",
            "           3      0.969     0.988     0.978       656\n",
            "           4      0.969     0.976     0.973       638\n",
            "           5      0.994     0.994     0.994       674\n",
            "           6      0.984     0.973     0.979       702\n",
            "           7      0.994     0.991     0.993       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.978     1.000     0.989       671\n",
            "          10      0.987     0.989     0.988       708\n",
            "          11      0.994     0.990     0.992       712\n",
            "          12      0.685     0.658     0.671       664\n",
            "          13      0.982     1.000     0.991       702\n",
            "          14      0.691     0.740     0.715       718\n",
            "\n",
            "    accuracy                          0.939     10229\n",
            "   macro avg      0.940     0.940     0.939     10229\n",
            "weighted avg      0.939     0.939     0.939     10229\n",
            "\n",
            "Top-k Model explanation score report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.272     0.384     0.318       703\n",
            "           1      0.947     0.600     0.734       687\n",
            "           2      0.894     0.973     0.932       638\n",
            "           3      0.911     0.840     0.874       656\n",
            "           4      0.890     0.687     0.775       638\n",
            "           5      0.946     0.724     0.820       674\n",
            "           6      0.897     0.843     0.869       702\n",
            "           7      0.565     0.991     0.720       693\n",
            "           8      1.000     1.000     1.000       663\n",
            "           9      0.961     0.879     0.918       671\n",
            "          10      0.942     0.527     0.676       708\n",
            "          11      0.763     0.990     0.862       712\n",
            "          12      0.000     0.000     0.000       664\n",
            "          13      0.743     0.813     0.777       702\n",
            "          14      0.531     0.926     0.675       718\n",
            "\n",
            "    accuracy                          0.746     10229\n",
            "   macro avg      0.751     0.745     0.730     10229\n",
            "weighted avg      0.749     0.746     0.729     10229\n",
            "\n",
            "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
            "Done!\n",
            "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
            "\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                 Classification Trust Report                                                                                                                 |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                    +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                    |\n",
            "|                                    |                                                                                      Summary                                                                                      |                                    |\n",
            "|                                    +-----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------+                                    |\n",
            "|                                    |                          Blackbox                         |                          Whitebox                         |                       Top-k Whitebox                      |                                    |\n",
            "|                                    +-----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------+                                    |\n",
            "|                                    |               Model:       RandomForestClassifier         |        Explanation method:         Trustee                |        Explanation method:         Trustee                |                                    |\n",
            "|                                    |           Dataset size:            34096                  |               Model:        DecisionTreeClassifier        |               Model:        DecisionTreeClassifier        |                                    |\n",
            "|                                    |         Train/Test Split:     70.00% / 30.00%             |            Iterations:                3                   |            Iterations:                3                   |                                    |\n",
            "|                                    |                                                           |            Sample size:             30.00%                |            Sample size:             30.00%                |                                    |\n",
            "|                                    |                                                           |                                                           |                                                           |                                    |\n",
            "|                                    |                                                           |         Decision Tree Info                                |         Decision Tree Info                                |                                    |\n",
            "|                                    |                                                           |               Size:                  395                  |               Size:                   47                  |                                    |\n",
            "|                                    |                                                           |               Depth:                  26                  |               Depth:                  12                  |                                    |\n",
            "|                                    |                                                           |              Leaves:                 198                  |              Leaves:                  24                  |                                    |\n",
            "|                                    |         # Input features:            77                   |         # Input features:        40 (51.95%)              |               Top-k:                  10                  |                                    |\n",
            "|                                    |         # Output classes:            15                   |         # Output classes:        15 (100.00%)             |         # Input features:             -                   |                                    |\n",
            "|                                    |                                                           |                                                           |         # Output classes:        15 (100.00%)             |                                    |\n",
            "|                                    |                                                           |                                                           |                                                           |                                    |\n",
            "|                                    | +-------------------------------------------------------+ | +-------------------------------------------------------+ | +-------------------------------------------------------+ |                                    |\n",
            "|                                    | |                      Performance                      | | |                        Fidelity                       | | |                        Fidelity                       | |                                    |\n",
            "|                                    | +-------------------------------------------------------+ | +-------------------------------------------------------+ | +-------------------------------------------------------+ |                                    |\n",
            "|                                    | |                                                       | | |                                                       | | |                                                       | |                                    |\n",
            "|                                    | |               precision    recall  f1-score   support | | |               precision    recall  f1-score   support | | |               precision    recall  f1-score   support | |                                    |\n",
            "|                                    | |                                                       | | |                                                       | | |                                                       | |                                    |\n",
            "|                                    | |            0      0.989     0.979     0.984       703 | | |            0      0.945     0.871     0.907       696 | | |            0      0.471     0.386     0.425       696 | |                                    |\n",
            "|                                    | |            1      0.993     1.000     0.996       687 | | |            1      0.973     0.997     0.985       692 | | |            1      0.873     1.000     0.932       692 | |                                    |\n",
            "|                                    | |            2      0.998     0.998     0.998       638 | | |            2      0.992     0.995     0.994       638 | | |            2      0.992     0.995     0.994       638 | |                                    |\n",
            "|                                    | |            3      0.995     0.998     0.997       656 | | |            3      0.970     0.974     0.972       658 | | |            3      0.758     0.872     0.811       658 | |                                    |\n",
            "|                                    | |            4      0.989     0.998     0.994       638 | | |            4      0.969     0.958     0.963       644 | | |            4      0.649     0.705     0.676       644 | |                                    |\n",
            "|                                    | |            5      0.999     0.994     0.996       674 | | |            5      0.985     0.997     0.991       671 | | |            5      0.867     0.729     0.792       671 | |                                    |\n",
            "|                                    | |            6      0.997     0.999     0.998       702 | | |            6      0.976     0.972     0.974       703 | | |            6      0.926     0.815     0.867       703 | |                                    |\n",
            "|                                    | |            7      1.000     1.000     1.000       693 | | |            7      0.997     0.997     0.997       693 | | |            7      0.983     1.000     0.991       693 | |                                    |\n",
            "|                                    | |            8      1.000     1.000     1.000       663 | | |            8      0.997     1.000     0.998       663 | | |            8      0.997     1.000     0.998       663 | |                                    |\n",
            "|                                    | |            9      1.000     1.000     1.000       671 | | |            9      0.997     1.000     0.999       671 | | |            9      0.997     1.000     0.999       671 | |                                    |\n",
            "|                                    | |           10      1.000     0.997     0.999       708 | | |           10      0.994     0.982     0.988       706 | | |           10      0.964     0.922     0.943       706 | |                                    |\n",
            "|                                    | |           11      1.000     0.996     0.998       712 | | |           11      0.985     1.000     0.992       709 | | |           11      0.985     1.000     0.992       709 | |                                    |\n",
            "|                                    | |           12      0.967     0.878     0.920       664 | | |           12      0.639     0.680     0.659       603 | | |           12      0.538     0.070     0.123       603 | |                                    |\n",
            "|                                    | |           13      1.000     1.000     1.000       702 | | |           13      0.972     1.000     0.986       702 | | |           13      0.918     1.000     0.957       702 | |                                    |\n",
            "|                                    | |           14      0.896     0.974     0.933       718 | | |           14      0.733     0.705     0.719       780 | | |           14      0.564     0.944     0.706       780 | |                                    |\n",
            "|                                    | |                                                       | | |                                                       | | |                                                       | |                                    |\n",
            "|                                    | |     accuracy                          0.987     10229 | | |     accuracy                          0.941     10229 | | |     accuracy                          0.836     10229 | |                                    |\n",
            "|                                    | |    macro avg      0.988     0.987     0.988     10229 | | |    macro avg      0.942     0.942     0.942     10229 | | |    macro avg      0.832     0.829     0.814     10229 | |                                    |\n",
            "|                                    | | weighted avg      0.988     0.987     0.987     10229 | | | weighted avg      0.942     0.941     0.941     10229 | | | weighted avg      0.832     0.836     0.818     10229 | |                                    |\n",
            "|                                    | |                                                       | | |                                                       | | |                                                       | |                                    |\n",
            "|                                    | +-------------------------------------------------------+ | +-------------------------------------------------------+ | +-------------------------------------------------------+ |                                    |\n",
            "|                                    +-----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------+                                    |\n",
            "| +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |\n",
            "| |                                                                                                                   Single-run Analysis                                                                                                                   | |\n",
            "| +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |\n",
            "| |                                                                                              +------------------------------------------------------------+                                                                                             | |\n",
            "| |                                                                                              |                      Top 10 Features                       |                                                                                             | |\n",
            "| |                                                                                              +------------------------+----------------+------------------+                                                                                             | |\n",
            "| |                                                                                              |        Feature         | # of Nodes (%) | Data Split % - ↓ |                                                                                             | |\n",
            "| |                                                                                              +------------------------+----------------+------------------+                                                                                             | |\n",
            "| |                                                                                              |    Destination Port    |   7 (3.55%)    |  10648 (32.61%)  |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |  Avg Bwd Segment Size  |   4 (2.03%)    |  3270 (10.02%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |      Fwd IAT Min       |  32 (16.24%)   |   2402 (7.36%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |     Bwd Packets/s      |   8 (4.06%)    |   2378 (7.28%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |  min_seg_size_forward  |   1 (0.51%)    |   2058 (6.30%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |      Flow IAT Min      |  35 (17.77%)   |   1957 (5.99%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |     Flow Duration      |   5 (2.54%)    |   1316 (4.03%)   |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              | Init_Win_bytes_forward |   7 (3.55%)    |   850 (2.60%)    |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              | Fwd Packet Length Max  |   5 (2.54%)    |   830 (2.54%)    |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |      Flow IAT Std      |   2 (1.02%)    |   653 (2.00%)    |                                                                                             | |\n",
            "| |                                                                                              |                        |                |                  |                                                                                             | |\n",
            "| |                                                                                              |       ----------       |   ----------   |    ----------    |                                                                                             | |\n",
            "| |                                                                                              |     Top 10 Summary     |  106 (53.81%)  |      80.74%      |                                                                                             | |\n",
            "| |                                                                                              +------------------------+----------------+------------------+                                                                                             | |\n",
            "| |  +-----------------------------------------------------------------------------------------------------------------+  +------------------------------------------------------------------------------------------------------------------------------+  | |\n",
            "| |  |                                                   Top 10 Nodes                                                  |  |                                                       Top 10 Branches                                                        |  | |\n",
            "| |  +--------------------------------+-----------------+------------------+-------------------------------------------+  +------------------------------------------+-------------------------------+-----------------+---------------------------------+  | |\n",
            "| |  | Decision                       | Gini  Split - ↓ | Data Split % - ↓ | Data Split % by Class (L/R)               |  | Rule                                     | Decision (P(x))               | Samples (%) - ↓ | Class Samples (%)               |  | |\n",
            "| |  +--------------------------------+-----------------+------------------+-------------------------------------------+  +------------------------------------------+-------------------------------+-----------------+---------------------------------+  | |\n",
            "| |  |                                |                 |                  | BENIGN: 0.44% / 99.56%                    |  | Destination Port <= 21.5                 | ('FTP-Patator',)              | 254             |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 100.00%                      |  |  and Subflow Fwd Packets > 1.5           | (0.00%)                       | (7.24%)         | 100.00%                         |  | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 100.00%                     |  |  and Avg Bwd Segment Size <= 15.20703125 |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 0.00% / 100.00%            |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 0.00% / 100.00%                 |  | Destination Port > 21.5                  | ('SSH-Patator',)              | 251             | 100.00%                         |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 0.00% / 100.00%         |  |  and Destination Port <= 27.5            | (0.00%)                       | (7.16%)         |                                 |  | |\n",
            "| |  |                                | Left: 0.05      | Left: 7.41%      | DoS slowloris: 0.00% / 100.00%            |  |                                          |                               |                 |                                 |  | |\n",
            "| |  | Destination Port <= 21.5       | Right: 0.93     | Right: 92.59%    | FTP-Patator: 100.00% / 0.00%              |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 100.00%               |  |  and Destination Port > 27.5             | ('Infiltration',)             | 227             |                                 |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 100.00%             |  |  and Avg Bwd Segment Size <= 3304.0      | (0.00%)                       | (6.47%)         | 100.00%                         |  | |\n",
            "| |  |                                |                 |                  | PortScan: 2.13% / 97.87%                  |  |  and Destination Port > 443.5            |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 100.00%              |  |  and Destination Port <= 451.0           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 100.00%   |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 100.00% |  | Destination Port > 21.5                  | ('Heartbleed',)               | 221             |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 0.00% / 100.00%           |  |  and Destination Port > 27.5             | (0.00%)                       | (6.30%)         | 100.00%                         |  | |\n",
            "| |  |                                |                 |                  |                                           |  |  and Avg Bwd Segment Size > 3304.0       |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 0.00% / 99.56%                    |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 100.00%                      |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 100.00%                     |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 0.00% / 100.00%            |  |  and Avg Bwd Segment Size <= 3304.0      | ('DDoS',)                     | 220             |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 0.00% / 100.00%                 |  |  and Destination Port <= 443.5           | (0.00%)                       | (6.27%)         | 100.00%                         |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 0.00% / 100.00%         |  |  and min_seg_size_forward <= 22.0        |                               |                 |                                 |  | |\n",
            "| |  |                                | Left: 0.00      | Left: 7.16%      | DoS slowloris: 0.00% / 100.00%            |  |  and Fwd Packet Length Max <= 23.5       |                               |                 |                                 |  | |\n",
            "| |  | Destination Port <= 27.5       | Right: 0.92     | Right: 85.43%    | FTP-Patator: 0.00% / 0.00%                |  |  and Subflow Fwd Bytes > 19.0            |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 100.00%               |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 100.00%             |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | PortScan: 0.00% / 97.87%                  |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 100.00% / 0.00%              |  |  and Avg Bwd Segment Size <= 3304.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 100.00%   |  |  and Destination Port <= 443.5           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 100.00% |  |  and min_seg_size_forward > 22.0         |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 0.00% / 100.00%           |  |  and Bwd Packets/s <= 0.108551025390625  | ('DoS slowloris',)            | 194             | 80.17%                          |  | |\n",
            "| |  |                                |                 |                  |                                           |  |  and Active Min <= 3631.5                | (0.00%)                       | (5.53%)         |                                 |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 99.56% / 0.00%                    |  |  and Flow IAT Min > 3.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 100.00% / 0.00%                      |  |  and Flow IAT Std > 533860.6875          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DDoS: 100.00% / 0.00%                     |  |  and Fwd Packet Length Max <= 288.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 100.00% / 0.00%            |  |  and Bwd Packets/s <= 0.04266357421875   |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 100.00% / 0.00%                 |  |  and Init_Win_bytes_forward <= 47367.5   |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 100.00% / 0.00%         |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                | Left: 0.92      | Left: 79.13%     | DoS slowloris: 100.00% / 0.00%            |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  | Avg Bwd Segment Size <= 3304.0 | Right: 0.00     | Right: 6.30%     | FTP-Patator: 0.00% / 0.00%                |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 100.00%               |  |  and Avg Bwd Segment Size <= 3304.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 100.00% / 0.00%             |  |  and Destination Port > 443.5            |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | PortScan: 97.87% / 0.00%                  |  |  and Destination Port > 451.0            | ('Bot',)                      | 174             | 75.65%                          |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |  |  and Average Packet Size > 8.0           | (0.00%)                       | (4.96%)         |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 100.00% / 0.00%   |  |  and Total Length of Bwd Packets > 3.0   |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 100.00% / 0.00% |  |  and Init_Win_bytes_forward > 233.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 100.00% / 0.00%           |  |  and Flow IAT Mean <= 1729443.33203125   |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  |                                           |  |  and Destination Port <= 56161.0         |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 37.72% / 41.23%                   |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 0.00%                        |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DDoS: 100.00% / 0.00%                     |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 0.00% / 100.00%            |  |  and Avg Bwd Segment Size <= 3304.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 41.51% / 58.49%                 |  |  and Destination Port <= 443.5           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 0.00% / 100.00%         |  |  and min_seg_size_forward > 22.0         |                               |                 |                                 |  | |\n",
            "| |  |                                | Left: 0.59      | Left: 11.26%     | DoS slowloris: 0.00% / 100.00%            |  |  and Bwd Packets/s > 0.108551025390625   |                               |                 |                                 |  | |\n",
            "| |  | min_seg_size_forward <= 22.0   | Right: 0.87     | Right: 47.42%    | FTP-Patator: 0.00% / 0.00%                |  |  and Fwd IAT Min <= 547.5                |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |  |  and Flow Duration <= 6727907.5          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 0.00%               |  |  and Flow Bytes/s <= 984.1285705566406   | ('Web Attack Sql Injection',) | 167             | 75.57%                          |  | |\n",
            "| |  |                                |                 |                  | PortScan: 0.43% / 6.81%                   |  |  and Init_Win_bytes_backward <= 239.5    | (0.00%)                       | (4.76%)         |                                 |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |  |  and Init_Win_bytes_forward > 247.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 100.00%   |  |  and Bwd IAT Min <= 1601.0               |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 100.00% |  |  and Destination Port <= 261.5           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 0.00% / 100.00%           |  |  and Flow Duration <= 5087443.5          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  |                                           |  |  and Flow IAT Min > 2.0                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 78.95% / 20.61%                   |  |  and Fwd IAT Min <= 369.5                |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 100.00%                      |  |  and Fwd IAT Min <= 81.5                 |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DDoS: 100.00% / 0.00%                     |  |  and Max Packet Length <= 3112.0         |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 100.00% / 0.00%            |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 100.00% / 0.00%                 |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 100.00% / 0.00%         |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                | Left: 0.89      | Left: 58.68%     | DoS slowloris: 100.00% / 0.00%            |  |  and Avg Bwd Segment Size <= 3304.0      |                               |                 |                                 |  | |\n",
            "| |  | Destination Port <= 443.5      | Right: 0.70     | Right: 20.44%    | FTP-Patator: 0.00% / 0.00%                |  |  and Destination Port <= 443.5           | ('DoS GoldenEye',)            | 159             |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |  |  and min_seg_size_forward > 22.0         | (0.00%)                       | (4.53%)         | 65.43%                          |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 100.00%             |  |  and Bwd Packets/s > 0.108551025390625   |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | PortScan: 7.23% / 90.64%                  |  |  and Fwd IAT Min <= 547.5                |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |  |  and Flow Duration > 6727907.5           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 100.00% / 0.00%   |  |  and Bwd Packet Length Std > 902.75      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 100.00% / 0.00% |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 100.00% / 0.00%           |  | Destination Port > 21.5                  |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  |                                           |  |  and Destination Port > 27.5             |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 0.00% / 20.61%                    |  |  and Avg Bwd Segment Size <= 3304.0      |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 100.00%                      |  |  and Destination Port > 443.5            | ('PortScan',)                 | 134             |                                 |  | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 0.00%                       |  |  and Destination Port > 451.0            | (0.00%)                       | (3.82%)         | 57.02%                          |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 0.00% / 0.00%              |  |  and Average Packet Size <= 8.0          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 0.00% / 0.00%                   |  |  and Bwd Header Length <= 26.0           |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 0.00% / 0.00%           |  |  and Total Length of Bwd Packets > 3.0   |                               |                 |                                 |  | |\n",
            "| |  |                                | Left: 0.00      | Left: 6.47%      | DoS slowloris: 0.00% / 0.00%              |  |  and Flow IAT Max <= 51.5                |                               |                 |                                 |  | |\n",
            "| |  | Destination Port <= 451.0      | Right: 0.58     | Right: 13.97%    | FTP-Patator: 0.00% / 0.00%                |  |                                          |                               |                 |                                 |  | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |  | ----------                               | ----------                    | ----------      | ----------                      |  | |\n",
            "| |  |                                |                 |                  | Infiltration: 100.00% / 0.00%             |  |                                          |                               |                 | FTP-Patator:100.00%             |  | |\n",
            "| |  |                                |                 |                  | PortScan: 0.00% / 90.64%                  |  |                                          |                               |                 | SSH-Patator:100.00%             |  | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |  |                                          |                               |                 | Infiltration:100.00%            |  | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 0.00%     |  |                                          |                               |                 | Heartbleed:100.00%              |  | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 0.00%   |  | Top 10 Summary                           | -                             | 2001 (57.06%)   | DDoS:100.00%                    |  | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 0.00% / 0.00%             |  |                                          |                               |                 | DoS slowloris:80.17%            |  | |\n",
            "| |  |                                |                 |                  |                                           |  |                                          |                               |                 | Bot:75.65%                      |  | |\n",
            "| |  |                                |                 |                  | BENIGN: 4.39% / 0.00%                     |  |                                          |                               |                 | Web Attack Sql Injection:75.57% |  | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 0.00%                        |  |                                          |                               |                 | DoS GoldenEye:65.43%            |  | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 0.00%                       |  |                                          |                               |                 | PortScan:57.02%                 |  | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 29.22% / 0.00%             |  +------------------------------------------+-------------------------------+-----------------+---------------------------------+  | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 51.89% / 0.00%                  |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 20.91% / 63.18%         |                                                                                                                                    | |\n",
            "| |  |                                | Left: 0.70      | Left: 12.72%     | DoS slowloris: 83.88% / 2.48%             |                                                                                                                                    | |\n",
            "| |  | Active Min <= 3631.5           | Right: 0.08     | Right: 4.13%     | FTP-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 0.00%               |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | PortScan: 0.43% / 0.00%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 0.00%     |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 0.00%   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 1.90% / 0.00%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  |                                           |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | BENIGN: 31.58% / 4.82%                    |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 0.00%                        |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 0.00%                       |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 4.53% / 65.43%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 1.89% / 0.00%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 15.91% / 0.00%          |                                                                                                                                    | |\n",
            "| |  |                                | Left: 0.67      | Left: 11.89%     | DoS slowloris: 13.22% / 0.00%             |                                                                                                                                    | |\n",
            "| |  | Flow Duration <= 6727907.5     | Right: 0.15     | Right: 4.93%     | FTP-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 0.00%               |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | PortScan: 6.38% / 0.00%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 9.58% / 0.00%     |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 100.00% / 0.00% |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 1.52% / 1.14%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  |                                           |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | BENIGN: 36.40% / 0.44%                    |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 0.00%                        |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 0.00%                       |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 69.96% / 0.82%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 1.89% / 4.72%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 15.91% / 0.00%          |                                                                                                                                    | |\n",
            "| |  |                                | Left: 0.75      | Left: 16.82%     | DoS slowloris: 13.22% / 0.41%             |                                                                                                                                    | |\n",
            "| |  | Fwd IAT Min <= 547.5           | Right: 0.53     | Right: 13.74%    | FTP-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 0.00%               |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | PortScan: 6.38% / 0.00%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 9.58% / 90.42%    |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 100.00% / 0.00% |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 2.66% / 95.44%            |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  |                                           |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | BENIGN: 0.44% / 3.95%                     |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Bot: 0.00% / 0.00%                        |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DDoS: 0.00% / 0.00%                       |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS GoldenEye: 0.00% / 29.22%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Hulk: 41.04% / 10.85%                 |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | DoS Slowhttptest: 1.36% / 19.55%          |                                                                                                                                    | |\n",
            "| |  |                                | Left: 0.12      | Left: 2.65%      | DoS slowloris: 0.83% / 83.06%             |                                                                                                                                    | |\n",
            "| |  | Flow IAT Min <= 3.5            | Right: 0.62     | Right: 10.07%    | FTP-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Heartbleed: 0.00% / 0.00%                 |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Infiltration: 0.00% / 0.00%               |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | PortScan: 0.00% / 0.43%                   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | SSH-Patator: 0.00% / 0.00%                |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Brute Force: 0.00% / 0.00%     |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack Sql Injection: 0.00% / 0.00%   |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  | Web Attack XSS: 0.00% / 1.90%             |                                                                                                                                    | |\n",
            "| |  |                                |                 |                  |                                           |                                                                                                                                    | |\n",
            "| |  +--------------------------------+-----------------+------------------+-------------------------------------------+                                                                                                                                    | |\n",
            "| +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |\n",
            "|                                   +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                  |\n",
            "|                                   |                                                                                  Prunning Analysis                                                                                   |                                  |\n",
            "|                                   +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                  |\n",
            "|                                   |              +--------------------------------------------------------------------------------------------------------------------------------------------------------+              |                                  |\n",
            "|                                   |              |                                                                Trustee Top-k Iteration                                                                 |              |                                  |\n",
            "|                                   |              +---+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+              |                                  |\n",
            "|                                   |              | k | DT Size | DT Depth | DT Num Leaves | Performance                                           | Fidelity                                              |              |                                  |\n",
            "|                                   |              +---+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |              |                                  |\n",
            "|                                   |              | 1 | 3       | 1        | 2             |            8      0.000     0.000     0.000       663 |            8      0.000     0.000     0.000       663 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           11      0.000     0.000     0.000       712 |           11      0.000     0.000     0.000       709 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           14      0.075     1.000     0.140       718 |           14      0.082     1.000     0.151       780 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |     accuracy                          0.138     10229 |     accuracy                          0.144     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |    macro avg      0.071     0.133     0.075     10229 |    macro avg      0.071     0.133     0.076     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               | weighted avg      0.072     0.138     0.077     10229 | weighted avg      0.073     0.144     0.079     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |              |                                  |\n",
            "|                                   |              | 2 | 5       | 2        | 3             |            8      0.000     0.000     0.000       663 |            8      0.000     0.000     0.000       663 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           11      0.989     1.000     0.994       712 |           11      0.985     1.000     0.992       709 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           14      0.082     1.000     0.151       718 |           14      0.089     1.000     0.163       780 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |     accuracy                          0.208     10229 |     accuracy                          0.213     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |    macro avg      0.137     0.200     0.142     10229 |    macro avg      0.137     0.200     0.143     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               | weighted avg      0.141     0.208     0.147     10229 | weighted avg      0.142     0.213     0.148     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            1      0.460     1.000     0.630       687 |            1      0.463     1.000     0.633       692 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |              |                                  |\n",
            "|                                   |              | 3 | 11      | 5        | 6             |            8      0.997     1.000     0.998       663 |            8      0.997     1.000     0.998       663 |              |                                  |\n",
            "|                                   |              |   |         |          |               |            9      0.997     1.000     0.999       671 |            9      0.997     1.000     0.999       671 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           11      0.989     1.000     0.994       712 |           11      0.985     1.000     0.992       709 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |              |                                  |\n",
            "|                                   |              |   |         |          |               |           14      0.120     1.000     0.215       718 |           14      0.131     1.000     0.231       780 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |     accuracy                          0.405     10229 |     accuracy                          0.411     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |    macro avg      0.303     0.400     0.322     10229 |    macro avg      0.304     0.400     0.323     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               | weighted avg      0.305     0.405     0.324     10229 | weighted avg      0.306     0.411     0.327     10229 |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              |   |         |          |               |                                                       |                                                       |              |                                  |\n",
            "|                                   |              +---+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+              |                                  |\n",
            "|                                   | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |                                  |\n",
            "|                                   | |                                                                               CCP Alpha Iteration                                                                                | |                                  |\n",
            "|                                   | +---------------------+-------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+ |                                  |\n",
            "|                                   | | Alpha               | Gini  | DT Size | DT Depth | DT Num Leaves | Performance                                           | Fidelity                                              | |                                  |\n",
            "|                                   | +---------------------+-------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+ |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            6      0.086     1.000     0.159       702 |            6      0.086     1.000     0.159       703 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 | |                                  |\n",
            "|                                   | | 0.06409442280962374 | 0.000 | 7       | 3        | 4             |            8      0.997     1.000     0.998       663 |            8      0.997     1.000     0.998       663 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           11      0.993     1.000     0.997       712 |           11      0.989     1.000     0.994       709 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |     accuracy                          0.271     10229 |     accuracy                          0.271     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |    macro avg      0.204     0.267     0.210     10229 |    macro avg      0.204     0.267     0.210     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               | weighted avg      0.206     0.271     0.212     10229 | weighted avg      0.206     0.271     0.212     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 | |                                  |\n",
            "|                                   | | 0.06498617011676411 | 0.000 | 5       | 2        | 3             |            8      0.000     0.000     0.000       663 |            8      0.000     0.000     0.000       663 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            9      0.076     1.000     0.142       671 |            9      0.076     1.000     0.142       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           11      0.989     1.000     0.994       712 |           11      0.985     1.000     0.992       709 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |     accuracy                          0.203     10229 |     accuracy                          0.203     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |    macro avg      0.137     0.200     0.142     10229 |    macro avg      0.136     0.200     0.142     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               | weighted avg      0.140     0.203     0.146     10229 | weighted avg      0.140     0.203     0.145     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            5      0.077     1.000     0.142       674 |            5      0.076     1.000     0.142       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 | |                                  |\n",
            "|                                   | | 0.06646156966697392 | 0.000 | 5       | 2        | 3             |            8      0.000     0.000     0.000       663 |            8      0.000     0.000     0.000       663 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           11      0.989     1.000     0.994       712 |           11      0.985     1.000     0.992       709 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |     accuracy                          0.203     10229 |     accuracy                          0.203     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |    macro avg      0.137     0.200     0.142     10229 |    macro avg      0.136     0.200     0.142     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               | weighted avg      0.140     0.203     0.146     10229 | weighted avg      0.140     0.203     0.145     10229 | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | |                     |       |         |          |               |                                                       |                                                       | |                                  |\n",
            "|                                   | +---------------------+-------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+ |                                  |\n",
            "|                                   |          +----------------------------------------------------------------------------------------------------------------------------------------------------------------+          |                                  |\n",
            "|                                   |          |                                                                      Max Depth Iteration                                                                       |          |                                  |\n",
            "|                                   |          +-----------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   |          | Max Depth | DT Size | DT Depth | DT Num Leaves | Performance                                           | Fidelity                                              |          |                                  |\n",
            "|                                   |          +-----------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |            0      0.074     1.000     0.137       703 |            0      0.073     1.000     0.136       696 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            7      0.000     0.000     0.000       693 |            7      0.000     0.000     0.000       693 |          |                                  |\n",
            "|                                   |          | 1         | 3       | 1        | 2             |            8      0.993     1.000     0.996       663 |            8      0.993     1.000     0.996       663 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           11      0.000     0.000     0.000       712 |           11      0.000     0.000     0.000       709 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |     accuracy                          0.134     10229 |     accuracy                          0.133     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |    macro avg      0.071     0.133     0.076     10229 |    macro avg      0.071     0.133     0.075     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               | weighted avg      0.069     0.134     0.074     10229 | weighted avg      0.069     0.133     0.074     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            7      0.997     1.000     0.999       693 |            7      0.997     1.000     0.999       693 |          |                                  |\n",
            "|                                   |          | 2         | 7       | 2        | 4             |            8      0.998     1.000     0.999       663 |            8      0.998     1.000     0.999       663 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           10      1.000     0.014     0.028       708 |           10      1.000     0.014     0.028       706 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           11      0.000     0.000     0.000       712 |           11      0.000     0.000     0.000       709 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           14      0.081     1.000     0.150       718 |           14      0.088     1.000     0.162       780 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |     accuracy                          0.204     10229 |     accuracy                          0.210     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |    macro avg      0.205     0.201     0.145     10229 |    macro avg      0.206     0.201     0.146     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               | weighted avg      0.207     0.204     0.145     10229 | weighted avg      0.208     0.210     0.147     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |            0      1.000     0.001     0.003       703 |            0      1.000     0.001     0.003       696 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            7      0.997     0.997     0.997       693 |            7      0.997     0.997     0.997       693 |          |                                  |\n",
            "|                                   |          | 3         | 13      | 3        | 7             |            8      0.998     1.000     0.999       663 |            8      0.998     1.000     0.999       663 |          |                                  |\n",
            "|                                   |          |           |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           10      0.833     0.014     0.028       708 |           10      0.833     0.014     0.028       706 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           11      0.990     1.000     0.995       712 |           11      0.986     1.000     0.993       709 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           12      0.082     1.000     0.151       664 |           12      0.074     1.000     0.138       603 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |          |           |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |     accuracy                          0.268     10229 |     accuracy                          0.262     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |    macro avg      0.327     0.268     0.212     10229 |    macro avg      0.326     0.268     0.211     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               | weighted avg      0.333     0.268     0.213     10229 | weighted avg      0.331     0.262     0.211     10229 |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          |           |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |          +-----------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   |         +-----------------------------------------------------------------------------------------------------------------------------------------------------------------+          |                                  |\n",
            "|                                   |         |                                                                       Max Leaves Iteration                                                                      |          |                                  |\n",
            "|                                   |         +------------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   |         | Max Leaves | DT Size | DT Depth | DT Num Leaves | Performance                                           | Fidelity                                              |          |                                  |\n",
            "|                                   |         +------------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            5      0.071     1.000     0.132       674 |            5      0.070     1.000     0.132       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |          |                                  |\n",
            "|                                   |         | 2          | 3       | 1        | 2             |            8      0.000     0.000     0.000       663 |            8      0.000     0.000     0.000       663 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           11      0.000     0.000     0.000       712 |           11      0.000     0.000     0.000       709 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |     accuracy                          0.134     10229 |     accuracy                          0.133     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |    macro avg      0.070     0.133     0.075     10229 |    macro avg      0.070     0.133     0.075     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               | weighted avg      0.071     0.134     0.076     10229 | weighted avg      0.071     0.133     0.076     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            2      0.072     1.000     0.134       638 |            2      0.072     1.000     0.134       638 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            3      0.000     0.000     0.000       656 |            3      0.000     0.000     0.000       658 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |          |                                  |\n",
            "|                                   |         | 3          | 5       | 2        | 3             |            8      0.998     1.000     0.999       663 |            8      0.998     1.000     0.999       663 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           11      0.000     0.000     0.000       712 |           11      0.000     0.000     0.000       709 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |     accuracy                          0.195     10229 |     accuracy                          0.195     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |    macro avg      0.137     0.200     0.142     10229 |    macro avg      0.137     0.200     0.142     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               | weighted avg      0.136     0.195     0.140     10229 | weighted avg      0.136     0.195     0.140     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |               precision    recall  f1-score   support |               precision    recall  f1-score   support |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |            0      0.000     0.000     0.000       703 |            0      0.000     0.000     0.000       696 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            1      0.000     0.000     0.000       687 |            1      0.000     0.000     0.000       692 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            2      0.000     0.000     0.000       638 |            2      0.000     0.000     0.000       638 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            3      0.081     1.000     0.149       656 |            3      0.081     1.000     0.150       658 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            4      0.000     0.000     0.000       638 |            4      0.000     0.000     0.000       644 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            5      0.000     0.000     0.000       674 |            5      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            6      0.000     0.000     0.000       702 |            6      0.000     0.000     0.000       703 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            7      0.983     1.000     0.991       693 |            7      0.983     1.000     0.991       693 |          |                                  |\n",
            "|                                   |         | 4          | 7       | 3        | 4             |            8      0.998     1.000     0.999       663 |            8      0.998     1.000     0.999       663 |          |                                  |\n",
            "|                                   |         |            |         |          |               |            9      0.000     0.000     0.000       671 |            9      0.000     0.000     0.000       671 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           10      0.000     0.000     0.000       708 |           10      0.000     0.000     0.000       706 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           11      0.989     1.000     0.994       712 |           11      0.985     1.000     0.992       709 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           12      0.000     0.000     0.000       664 |           12      0.000     0.000     0.000       603 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           13      0.000     0.000     0.000       702 |           13      0.000     0.000     0.000       702 |          |                                  |\n",
            "|                                   |         |            |         |          |               |           14      0.000     0.000     0.000       718 |           14      0.000     0.000     0.000       780 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |     accuracy                          0.266     10229 |     accuracy                          0.266     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |    macro avg      0.203     0.267     0.209     10229 |    macro avg      0.203     0.267     0.209     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               | weighted avg      0.205     0.266     0.211     10229 | weighted avg      0.205     0.266     0.210     10229 |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         |            |         |          |               |                                                       |                                                       |          |                                  |\n",
            "|                                   |         +------------+---------+----------+---------------+-------------------------------------------------------+-------------------------------------------------------+          |                                  |\n",
            "|                                   +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                                  |\n",
            "|                            +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                            |\n",
            "|                            |                                                                                       Repeated-run Analysis                                                                                       |                            |\n",
            "|                            +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                            |\n",
            "|                            | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |                            |\n",
            "|                            | |                                                                                   Iterative Feature Removal                                                                                   | |                            |\n",
            "|                            | +-----------+-------------------------+--------------------+-------------------------------------------------------+--------------------+-------------------------------------------------------+ |                            |\n",
            "|                            | | Iteration | Feature Removed         | # Features Removed | Performance                                           | Decision Tree Size | Fidelity                                              | |                            |\n",
            "|                            | +-----------+-------------------------+--------------------+-------------------------------------------------------+--------------------+-------------------------------------------------------+ |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |               precision    recall  f1-score   support |                    |               precision    recall  f1-score   support | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |            0      0.981     0.962     0.971       703 |                    |            0      0.937     0.865     0.900       689 | |                            |\n",
            "|                            | |           |                         |                    |            1      0.993     0.994     0.993       687 |                    |            1      0.970     0.996     0.983       688 | |                            |\n",
            "|                            | |           |                         |                    |            2      1.000     0.998     0.999       638 |                    |            2      0.998     0.995     0.997       637 | |                            |\n",
            "|                            | |           |                         |                    |            3      0.992     0.998     0.995       656 |                    |            3      0.979     0.991     0.985       660 | |                            |\n",
            "|                            | |           |                         |                    |            4      0.989     0.998     0.994       638 |                    |            4      0.992     0.966     0.979       644 | |                            |\n",
            "|                            | |           |                         |                    |            5      0.999     0.994     0.996       674 |                    |            5      0.987     0.993     0.990       671 | |                            |\n",
            "|                            | |           |                         |                    |            6      0.994     0.999     0.996       702 |                    |            6      0.994     0.984     0.989       705 | |                            |\n",
            "|                            | |           |                         |                    |            7      0.997     0.999     0.998       693 |                    |            7      0.987     0.999     0.993       694 | |                            |\n",
            "|                            | | 0         | Destination Port        | 1                  |            8      1.000     1.000     1.000       663 | 427                |            8      1.000     1.000     1.000       663 | |                            |\n",
            "|                            | |           |                         |                    |            9      0.997     1.000     0.999       671 |                    |            9      0.982     1.000     0.991       673 | |                            |\n",
            "|                            | |           |                         |                    |           10      1.000     1.000     1.000       708 |                    |           10      0.994     0.989     0.992       708 | |                            |\n",
            "|                            | |           |                         |                    |           11      0.997     0.993     0.995       712 |                    |           11      0.987     0.994     0.991       709 | |                            |\n",
            "|                            | |           |                         |                    |           12      0.957     0.875     0.914       664 |                    |           12      0.607     0.728     0.662       607 | |                            |\n",
            "|                            | |           |                         |                    |           13      1.000     1.000     1.000       702 |                    |           13      0.992     1.000     0.996       702 | |                            |\n",
            "|                            | |           |                         |                    |           14      0.893     0.969     0.930       718 |                    |           14      0.747     0.652     0.696       779 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |     accuracy                          0.985     10229 |                    |     accuracy                          0.942     10229 | |                            |\n",
            "|                            | |           |                         |                    |    macro avg      0.986     0.985     0.985     10229 |                    |    macro avg      0.944     0.943     0.943     10229 | |                            |\n",
            "|                            | |           |                         |                    | weighted avg      0.986     0.985     0.985     10229 |                    | weighted avg      0.944     0.942     0.942     10229 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |               precision    recall  f1-score   support |                    |               precision    recall  f1-score   support | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |            0      0.981     0.933     0.956       703 |                    |            0      0.917     0.837     0.875       669 | |                            |\n",
            "|                            | |           |                         |                    |            1      0.983     0.994     0.988       687 |                    |            1      0.966     0.974     0.970       695 | |                            |\n",
            "|                            | |           |                         |                    |            2      0.998     0.998     0.998       638 |                    |            2      0.995     0.997     0.996       638 | |                            |\n",
            "|                            | |           |                         |                    |            3      0.989     0.998     0.994       656 |                    |            3      0.974     0.950     0.962       662 | |                            |\n",
            "|                            | |           |                         |                    |            4      0.992     0.997     0.995       638 |                    |            4      0.972     0.984     0.978       641 | |                            |\n",
            "|                            | |           |                         |                    |            5      0.999     0.994     0.996       674 |                    |            5      0.963     1.000     0.981       671 | |                            |\n",
            "|                            | |           |                         |                    |            6      0.993     0.999     0.996       702 |                    |            6      0.981     0.976     0.979       706 | |                            |\n",
            "|                            | |           |                         |                    |            7      0.997     0.999     0.998       693 |                    |            7      0.994     0.997     0.996       694 | |                            |\n",
            "|                            | | 1         | Init_Win_bytes_backward | 2                  |            8      1.000     1.000     1.000       663 | 427                |            8      1.000     1.000     1.000       663 | |                            |\n",
            "|                            | |           |                         |                    |            9      0.997     1.000     0.999       671 |                    |            9      0.978     1.000     0.989       673 | |                            |\n",
            "|                            | |           |                         |                    |           10      1.000     1.000     1.000       708 |                    |           10      0.994     0.994     0.994       708 | |                            |\n",
            "|                            | |           |                         |                    |           11      0.989     0.993     0.991       712 |                    |           11      0.985     0.987     0.986       715 | |                            |\n",
            "|                            | |           |                         |                    |           12      0.951     0.869     0.908       664 |                    |           12      0.613     0.761     0.679       607 | |                            |\n",
            "|                            | |           |                         |                    |           13      1.000     1.000     1.000       702 |                    |           13      0.987     1.000     0.994       702 | |                            |\n",
            "|                            | |           |                         |                    |           14      0.887     0.969     0.926       718 |                    |           14      0.762     0.628     0.689       785 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |     accuracy                          0.983     10229 |                    |     accuracy                          0.937     10229 | |                            |\n",
            "|                            | |           |                         |                    |    macro avg      0.984     0.983     0.983     10229 |                    |    macro avg      0.939     0.939     0.938     10229 | |                            |\n",
            "|                            | |           |                         |                    | weighted avg      0.983     0.983     0.983     10229 |                    | weighted avg      0.939     0.937     0.937     10229 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |               precision    recall  f1-score   support |                    |               precision    recall  f1-score   support | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |            0      0.982     0.933     0.957       703 |                    |            0      0.902     0.841     0.871       668 | |                            |\n",
            "|                            | |           |                         |                    |            1      0.983     0.994     0.988       687 |                    |            1      0.977     0.977     0.977       695 | |                            |\n",
            "|                            | |           |                         |                    |            2      1.000     0.998     0.999       638 |                    |            2      0.975     0.998     0.987       637 | |                            |\n",
            "|                            | |           |                         |                    |            3      0.991     0.998     0.995       656 |                    |            3      0.975     0.986     0.980       661 | |                            |\n",
            "|                            | |           |                         |                    |            4      0.989     0.997     0.993       638 |                    |            4      0.970     0.970     0.970       643 | |                            |\n",
            "|                            | |           |                         |                    |            5      0.999     0.994     0.996       674 |                    |            5      0.996     1.000     0.998       671 | |                            |\n",
            "|                            | |           |                         |                    |            6      0.993     0.999     0.996       702 |                    |            6      0.987     0.970     0.979       706 | |                            |\n",
            "|                            | |           |                         |                    |            7      0.997     0.999     0.998       693 |                    |            7      0.994     0.990     0.992       694 | |                            |\n",
            "|                            | | 2         | Fwd Packet Length Max   | 3                  |            8      1.000     1.000     1.000       663 | 439                |            8      1.000     1.000     1.000       663 | |                            |\n",
            "|                            | |           |                         |                    |            9      0.997     1.000     0.999       671 |                    |            9      0.978     0.997     0.987       673 | |                            |\n",
            "|                            | |           |                         |                    |           10      1.000     1.000     1.000       708 |                    |           10      0.987     0.989     0.988       708 | |                            |\n",
            "|                            | |           |                         |                    |           11      0.987     0.993     0.990       712 |                    |           11      0.993     0.983     0.988       716 | |                            |\n",
            "|                            | |           |                         |                    |           12      0.954     0.866     0.908       664 |                    |           12      0.657     0.695     0.675       603 | |                            |\n",
            "|                            | |           |                         |                    |           13      1.000     1.000     1.000       702 |                    |           13      0.982     1.000     0.991       702 | |                            |\n",
            "|                            | |           |                         |                    |           14      0.885     0.972     0.926       718 |                    |           14      0.749     0.729     0.739       789 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |     accuracy                          0.983     10229 |                    |     accuracy                          0.941     10229 | |                            |\n",
            "|                            | |           |                         |                    |    macro avg      0.984     0.983     0.983     10229 |                    |    macro avg      0.941     0.942     0.941     10229 | |                            |\n",
            "|                            | |           |                         |                    | weighted avg      0.983     0.983     0.983     10229 |                    | weighted avg      0.942     0.941     0.941     10229 | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | |           |                         |                    |                                                       |                    |                                                       | |                            |\n",
            "|                            | +-----------+-------------------------+--------------------+-------------------------------------------------------+--------------------+-------------------------------------------------------+ |                            |\n",
            "|                            +---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+                            |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "from trustee.report.trust import TrustReport\n",
        "\n",
        "trust_report = TrustReport(\n",
        "    blackbox,\n",
        "    X=X,\n",
        "    y=y,\n",
        "    top_k=10,\n",
        "    max_iter=3,\n",
        "    trustee_num_iter=3,\n",
        "    num_pruning_iter=3,\n",
        "    trustee_num_stability_iter=3,\n",
        "    trustee_sample_size=0.30,\n",
        "    analyze_stability=True,\n",
        "    skip_retrain=False,\n",
        "    feature_names=feature_names,\n",
        "    class_names=CIC_IDS_2017_DATASET_META[\"classes\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(trust_report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "OUTPUT_PATH = \"res/output\"\n",
        "trust_report.save(OUTPUT_PATH)\n",
        "\n",
        "# warning: execution time ~ 30 minutes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "HApsEvxQ0nuk",
        "outputId": "1f97337b-317a-4494-efdf-7eb03c4749f9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-72951a42df29>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"res/output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrust_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trustee/report/trust.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, output_dir, aggregate, save_all_dts)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{report_output_dir}/trust_report.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_dts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_all_dts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trustee/report/trust.py\u001b[0m in \u001b[0;36m__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"blackbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trustee\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: expert"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "DjZFmbyJQCKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}